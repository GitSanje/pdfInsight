Running RAG graph... <langgraph.graph.state.CompiledStateGraph object at 0x75b202930e60>

=== query_or_respond ===
[0] HumanMessage: What is the main contribution of the paper?
==============

state in query_or_respond:  {'messages': [HumanMessage(content='What is the main contribution of the paper?', additional_kwargs={}, response_metadata={}, id='2f3298bc-7a71-4983-9be0-fab643e0a237')]}
Response llm:  content='' additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{"query": "main contribution of the paper"}'}, '__gemini_function_call_thought_signatures__': {'174c90ec-fd4c-437d-8b18-106b8900a0e0': 'CpcDAXLI2nwPZq5U8FbXltUdhbse9tqEK6iLaf9FdTdzCc/bdFwDAmBwRKBKmbgiPhv0Rvo6remylvJjeib8pY1tCpUb/xXcivNw1a30qswjb0xG1qolje/VJo9MMULgG/ZrGPaYhVA0ul3hmbviKW0gg4bXCsemenFoK+GENGAF6G0UH6a5eaUooX4R645Gq5IbZ8XqLcqft+c43s6uOjI+20wBddfxECTFlz9LJ7avG1KtvNlZK90G3n0e+/Fx294E8uz9TYGbh6VN+6WsQTLicnSwMteHygUqcm8iPaHy1Ev2+TD0gr5LqP6ob+hyWsP4FC8cBxRfEC4Ap7YKCkQW9UPn35JhQHBAQHvkUG5xKsGsJmzhjV11FYPVvcp9TCmP0ZrINtpNhCIeSW69KjVQ5lO0Y9el+ERbwCjLhpn0Exa7W8tDWWw9OFST+QTGpHSwxvS0zGECv6rZB1ovdoqPXY4R82WznKrcIy+Y8ymmFcg1VZWs3WkA5wAdlxFSwwxRL6gsosz3lkRbGbu8nZURTNhqtuCbNSQ='}} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b53ec-1e28-7c53-b5c9-6087a1608759-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'main contribution of the paper'}, 'id': '174c90ec-fd4c-437d-8b18-106b8900a0e0', 'type': 'tool_call'}] usage_metadata={'input_tokens': 62, 'output_tokens': 95, 'total_tokens': 157, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 78}}

=== tools_condition ===
[0] HumanMessage: What is the main contribution of the paper?
[1] AIMessage: 
   tool_calls: [{'name': 'retrieve', 'args': {'query': 'main contribution of the paper'}, 'id': '174c90ec-fd4c-437d-8b18-106b8900a0e0', 'type': 'tool_call'}]
==============

state in tools_condition:  {'messages': [HumanMessage(content='What is the main contribution of the paper?', additional_kwargs={}, response_metadata={}, id='2f3298bc-7a71-4983-9be0-fab643e0a237'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{"query": "main contribution of the paper"}'}, '__gemini_function_call_thought_signatures__': {'174c90ec-fd4c-437d-8b18-106b8900a0e0': 'CpcDAXLI2nwPZq5U8FbXltUdhbse9tqEK6iLaf9FdTdzCc/bdFwDAmBwRKBKmbgiPhv0Rvo6remylvJjeib8pY1tCpUb/xXcivNw1a30qswjb0xG1qolje/VJo9MMULgG/ZrGPaYhVA0ul3hmbviKW0gg4bXCsemenFoK+GENGAF6G0UH6a5eaUooX4R645Gq5IbZ8XqLcqft+c43s6uOjI+20wBddfxECTFlz9LJ7avG1KtvNlZK90G3n0e+/Fx294E8uz9TYGbh6VN+6WsQTLicnSwMteHygUqcm8iPaHy1Ev2+TD0gr5LqP6ob+hyWsP4FC8cBxRfEC4Ap7YKCkQW9UPn35JhQHBAQHvkUG5xKsGsJmzhjV11FYPVvcp9TCmP0ZrINtpNhCIeSW69KjVQ5lO0Y9el+ERbwCjLhpn0Exa7W8tDWWw9OFST+QTGpHSwxvS0zGECv6rZB1ovdoqPXY4R82WznKrcIy+Y8ymmFcg1VZWs3WkA5wAdlxFSwwxRL6gsosz3lkRbGbu8nZURTNhqtuCbNSQ='}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b53ec-1e28-7c53-b5c9-6087a1608759-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'main contribution of the paper'}, 'id': '174c90ec-fd4c-437d-8b18-106b8900a0e0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 95, 'total_tokens': 157, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 78}})]}
Generating with retrieved context...

=== generate ===
[0] HumanMessage: What is the main contribution of the paper?
[1] AIMessage: 
   tool_calls: [{'name': 'retrieve', 'args': {'query': 'main contribution of the paper'}, 'id': '174c90ec-fd4c-437d-8b18-106b8900a0e0', 'type': 'tool_call'}]
[2] ToolMessage: Source: None
DeepSeek-OCR: Contexts Optical Compression
Haoran Wei, Yaofeng Sun, Yukun Li
DeepSeek-AI
Abstract
We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long
contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder
and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core
engine, designed to maintain low activations under high-resolution input while achieving high
compression ratios to ensure an optimal and manageable number of vision tokens. Experiments
show that when the number of text tokens is within 10 times that of vision tokens (i.e., a
compression ratio < 10×), the model can achieve decoding (OCR) precision of 97%. Even at a
compression ratio of 20×, the OCR accuracy still remains at about 60%. This shows considerable
promise for research areas such as historical long-context compression and memory forgetting
mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value.
On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens,
and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than
800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs
at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly
accessible athttp://github.com/deepseek-ai/DeepSeek-OCR.
600/uni00AD700700/uni00AD800800/uni00AD900900/uni00AD10001000/uni00AD11001100/uni00AD12001200/uni00AD1300
Text/uni00A0Tokens/uni00A0in/uni00A0Per/uni00A0Page/uni00A0(Ground/uni00ADtruth)
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%Precision/uni00A0(%)
96.5%
93.8%
83.8% 85.8%
79.3%
76.3%
59.1%
98.5% 97.3% 96.8% 96.8%
91.5% 89.8%
87.1%
64/uni00A0vis/uni00A0toks(left)100/uni00A0vis/uni00A0toks(left)64/uni00A0vis/uni00A0toks(right)100/uni00A0vis/uni00A0toks(right)
0x
5x
10x
15x
20x
Compression/uni00A0(×)
10.5
6.7
11.8
7.5
13.2
8.5
15.1
9.7
16.5
10.6
17.7
11.3
19.7
12.6
Source: None
tokens could achieve much higher compression ratios.
This insight motivates us to reexamine vision-language models (VLMs) from an LLM-centric
perspective, focusing on how vision encoders can enhance LLMs’ efficiency in processing textual
information rather than basic VQA [12, 16, 24, 32, 41] what humans excel at. OCR tasks, as an
intermediate modality bridging vision and language, provide an ideal testbed for this vision-
text compression paradigm, as they establish a natural compression-decompression mapping
between visual and textual representations while offering quantitative evaluation metrics.
Accordingly, we present DeepSeek-OCR, a VLM designed as a preliminary proof-of-concept
for efficient vision-text compression. Our work makes three primary contributions:
First, we provide comprehensive quantitative analysis of vision-text token compression
ratios. Our method achieves 96%+ OCR decoding precision at 9-10×text compression, ∼90% at
10-12×compression, and ∼60% at 20×compression on Fox [21] benchmarks featuring diverse
document layouts (with actual accuracy being even higher when accounting for formatting
differences between output and ground truth), as shown in Figure 1(a). The results demonstrate
that compact language models can effectively learn to decode compressed visual representations,
suggesting that larger LLMs could readily acquire similar capabilities through appropriate
pretraining design.
Second, we introduce DeepEncoder, a novel architecture that maintains low activation mem-
ory and minimal vision tokens even with high-resolution inputs. It serially connects window
attention and global attention encoder components through a 16×convolutional compressor.
This design ensures that the window attention component processes a large number of vision
tokens, while the compressor reduces vision tokens before they enter the dense global attention
component, achieving effective memory and token compression.
==============

state in generate:  {'messages': [HumanMessage(content='What is the main contribution of the paper?', additional_kwargs={}, response_metadata={}, id='2f3298bc-7a71-4983-9be0-fab643e0a237'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{"query": "main contribution of the paper"}'}, '__gemini_function_call_thought_signatures__': {'174c90ec-fd4c-437d-8b18-106b8900a0e0': 'CpcDAXLI2nwPZq5U8FbXltUdhbse9tqEK6iLaf9FdTdzCc/bdFwDAmBwRKBKmbgiPhv0Rvo6remylvJjeib8pY1tCpUb/xXcivNw1a30qswjb0xG1qolje/VJo9MMULgG/ZrGPaYhVA0ul3hmbviKW0gg4bXCsemenFoK+GENGAF6G0UH6a5eaUooX4R645Gq5IbZ8XqLcqft+c43s6uOjI+20wBddfxECTFlz9LJ7avG1KtvNlZK90G3n0e+/Fx294E8uz9TYGbh6VN+6WsQTLicnSwMteHygUqcm8iPaHy1Ev2+TD0gr5LqP6ob+hyWsP4FC8cBxRfEC4Ap7YKCkQW9UPn35JhQHBAQHvkUG5xKsGsJmzhjV11FYPVvcp9TCmP0ZrINtpNhCIeSW69KjVQ5lO0Y9el+ERbwCjLhpn0Exa7W8tDWWw9OFST+QTGpHSwxvS0zGECv6rZB1ovdoqPXY4R82WznKrcIy+Y8ymmFcg1VZWs3WkA5wAdlxFSwwxRL6gsosz3lkRbGbu8nZURTNhqtuCbNSQ='}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b53ec-1e28-7c53-b5c9-6087a1608759-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'main contribution of the paper'}, 'id': '174c90ec-fd4c-437d-8b18-106b8900a0e0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 95, 'total_tokens': 157, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 78}}), ToolMessage(content='Source: None\nDeepSeek-OCR: Contexts Optical Compression\nHaoran Wei, Yaofeng Sun, Yukun Li\nDeepSeek-AI\nAbstract\nWe present DeepSeek-OCR as an initial investigation into the feasibility of compressing long\ncontexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core\nengine, designed to maintain low activations under high-resolution input while achieving high\ncompression ratios to ensure an optimal and manageable number of vision tokens. Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%. Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%. This shows considerable\npromise for research areas such as historical long-context compression and memory forgetting\nmechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value.\nOn OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens,\nand outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than\n800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs\nat a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly\naccessible athttp://github.com/deepseek-ai/DeepSeek-OCR.\n600/uni00AD700700/uni00AD800800/uni00AD900900/uni00AD10001000/uni00AD11001100/uni00AD12001200/uni00AD1300\nText/uni00A0Tokens/uni00A0in/uni00A0Per/uni00A0Page/uni00A0(Ground/uni00ADtruth)\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%Precision/uni00A0(%)\n96.5%\n93.8%\n83.8% 85.8%\n79.3%\n76.3%\n59.1%\n98.5% 97.3% 96.8% 96.8%\n91.5% 89.8%\n87.1%\n64/uni00A0vis/uni00A0toks(left)100/uni00A0vis/uni00A0toks(left)64/uni00A0vis/uni00A0toks(right)100/uni00A0vis/uni00A0toks(right)\n0x\n5x\n10x\n15x\n20x\nCompression/uni00A0(×)\n10.5\n6.7\n11.8\n7.5\n13.2\n8.5\n15.1\n9.7\n16.5\n10.6\n17.7\n11.3\n19.7\n12.6\nSource: None\ntokens could achieve much higher compression ratios.\nThis insight motivates us to reexamine vision-language models (VLMs) from an LLM-centric\nperspective, focusing on how vision encoders can enhance LLMs’ efficiency in processing textual\ninformation rather than basic VQA [12, 16, 24, 32, 41] what humans excel at. OCR tasks, as an\nintermediate modality bridging vision and language, provide an ideal testbed for this vision-\ntext compression paradigm, as they establish a natural compression-decompression mapping\nbetween visual and textual representations while offering quantitative evaluation metrics.\nAccordingly, we present DeepSeek-OCR, a VLM designed as a preliminary proof-of-concept\nfor efficient vision-text compression. Our work makes three primary contributions:\nFirst, we provide comprehensive quantitative analysis of vision-text token compression\nratios. Our method achieves 96%+ OCR decoding precision at 9-10×text compression, ∼90% at\n10-12×compression, and ∼60% at 20×compression on Fox [21] benchmarks featuring diverse\ndocument layouts (with actual accuracy being even higher when accounting for formatting\ndifferences between output and ground truth), as shown in Figure 1(a). The results demonstrate\nthat compact language models can effectively learn to decode compressed visual representations,\nsuggesting that larger LLMs could readily acquire similar capabilities through appropriate\npretraining design.\nSecond, we introduce DeepEncoder, a novel architecture that maintains low activation mem-\nory and minimal vision tokens even with high-resolution inputs. It serially connects window\nattention and global attention encoder components through a 16×convolutional compressor.\nThis design ensures that the window attention component processes a large number of vision\ntokens, while the compressor reduces vision tokens before they enter the dense global attention\ncomponent, achieving effective memory and token compression.', name='retrieve', id='c1047d5b-38aa-4f9a-8a03-704cba819152', tool_call_id='174c90ec-fd4c-437d-8b18-106b8900a0e0')]}
{'messages': [HumanMessage(content='What is the main contribution of the paper?', additional_kwargs={}, response_metadata={}, id='2f3298bc-7a71-4983-9be0-fab643e0a237'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{"query": "main contribution of the paper"}'}, '__gemini_function_call_thought_signatures__': {'174c90ec-fd4c-437d-8b18-106b8900a0e0': 'CpcDAXLI2nwPZq5U8FbXltUdhbse9tqEK6iLaf9FdTdzCc/bdFwDAmBwRKBKmbgiPhv0Rvo6remylvJjeib8pY1tCpUb/xXcivNw1a30qswjb0xG1qolje/VJo9MMULgG/ZrGPaYhVA0ul3hmbviKW0gg4bXCsemenFoK+GENGAF6G0UH6a5eaUooX4R645Gq5IbZ8XqLcqft+c43s6uOjI+20wBddfxECTFlz9LJ7avG1KtvNlZK90G3n0e+/Fx294E8uz9TYGbh6VN+6WsQTLicnSwMteHygUqcm8iPaHy1Ev2+TD0gr5LqP6ob+hyWsP4FC8cBxRfEC4Ap7YKCkQW9UPn35JhQHBAQHvkUG5xKsGsJmzhjV11FYPVvcp9TCmP0ZrINtpNhCIeSW69KjVQ5lO0Y9el+ERbwCjLhpn0Exa7W8tDWWw9OFST+QTGpHSwxvS0zGECv6rZB1ovdoqPXY4R82WznKrcIy+Y8ymmFcg1VZWs3WkA5wAdlxFSwwxRL6gsosz3lkRbGbu8nZURTNhqtuCbNSQ='}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b53ec-1e28-7c53-b5c9-6087a1608759-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'main contribution of the paper'}, 'id': '174c90ec-fd4c-437d-8b18-106b8900a0e0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 95, 'total_tokens': 157, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 78}}), ToolMessage(content='Source: None\nDeepSeek-OCR: Contexts Optical Compression\nHaoran Wei, Yaofeng Sun, Yukun Li\nDeepSeek-AI\nAbstract\nWe present DeepSeek-OCR as an initial investigation into the feasibility of compressing long\ncontexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core\nengine, designed to maintain low activations under high-resolution input while achieving high\ncompression ratios to ensure an optimal and manageable number of vision tokens. Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%. Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%. This shows considerable\npromise for research areas such as historical long-context compression and memory forgetting\nmechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value.\nOn OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens,\nand outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than\n800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs\nat a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly\naccessible athttp://github.com/deepseek-ai/DeepSeek-OCR.\n600/uni00AD700700/uni00AD800800/uni00AD900900/uni00AD10001000/uni00AD11001100/uni00AD12001200/uni00AD1300\nText/uni00A0Tokens/uni00A0in/uni00A0Per/uni00A0Page/uni00A0(Ground/uni00ADtruth)\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%Precision/uni00A0(%)\n96.5%\n93.8%\n83.8% 85.8%\n79.3%\n76.3%\n59.1%\n98.5% 97.3% 96.8% 96.8%\n91.5% 89.8%\n87.1%\n64/uni00A0vis/uni00A0toks(left)100/uni00A0vis/uni00A0toks(left)64/uni00A0vis/uni00A0toks(right)100/uni00A0vis/uni00A0toks(right)\n0x\n5x\n10x\n15x\n20x\nCompression/uni00A0(×)\n10.5\n6.7\n11.8\n7.5\n13.2\n8.5\n15.1\n9.7\n16.5\n10.6\n17.7\n11.3\n19.7\n12.6\nSource: None\ntokens could achieve much higher compression ratios.\nThis insight motivates us to reexamine vision-language models (VLMs) from an LLM-centric\nperspective, focusing on how vision encoders can enhance LLMs’ efficiency in processing textual\ninformation rather than basic VQA [12, 16, 24, 32, 41] what humans excel at. OCR tasks, as an\nintermediate modality bridging vision and language, provide an ideal testbed for this vision-\ntext compression paradigm, as they establish a natural compression-decompression mapping\nbetween visual and textual representations while offering quantitative evaluation metrics.\nAccordingly, we present DeepSeek-OCR, a VLM designed as a preliminary proof-of-concept\nfor efficient vision-text compression. Our work makes three primary contributions:\nFirst, we provide comprehensive quantitative analysis of vision-text token compression\nratios. Our method achieves 96%+ OCR decoding precision at 9-10×text compression, ∼90% at\n10-12×compression, and ∼60% at 20×compression on Fox [21] benchmarks featuring diverse\ndocument layouts (with actual accuracy being even higher when accounting for formatting\ndifferences between output and ground truth), as shown in Figure 1(a). The results demonstrate\nthat compact language models can effectively learn to decode compressed visual representations,\nsuggesting that larger LLMs could readily acquire similar capabilities through appropriate\npretraining design.\nSecond, we introduce DeepEncoder, a novel architecture that maintains low activation mem-\nory and minimal vision tokens even with high-resolution inputs. It serially connects window\nattention and global attention encoder components through a 16×convolutional compressor.\nThis design ensures that the window attention component processes a large number of vision\ntokens, while the compressor reduces vision tokens before they enter the dense global attention\ncomponent, achieving effective memory and token compression.', name='retrieve', id='c1047d5b-38aa-4f9a-8a03-704cba819152', tool_call_id='174c90ec-fd4c-437d-8b18-106b8900a0e0'), AIMessage(content='The main contributions of the paper are:\n\n1.  **DeepSeek-OCR as a proof-of-concept for efficient vision-text compression:** It introduces DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping.\n2.  **Comprehensive quantitative analysis of vision-text token compression ratios:** The method achieves high OCR decoding precision, such as 96%+ at 9-10× text compression, ~90% at 10-12× compression, and ~60% at 20× compression on benchmarks.\n3.  **Introduction of DeepEncoder:** A novel architecture that maintains low activation memory and minimal vision tokens even with high-resolution inputs. It uses a 16× convolutional compressor to serially connect window attention and global attention encoder components, reducing vision tokens before global attention.\n4.  **Demonstration of high practical value:** DeepSeek-OCR surpasses existing OCR models like GOT-OCR2.0 and MinerU2.0 with fewer vision tokens and can generate training data for LLMs/VLMs at a large scale.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b53ec-2a11-7461-b683-cf9b9624ba7a-0', usage_metadata={'input_tokens': 1256, 'output_tokens': 1001, 'total_tokens': 2257, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 768}})]}





Running RAG graph... <langgraph.graph.state.CompiledStateGraph object at 0x71eb65659160>

=== query_or_respond ===
[0] HumanMessage: describe  table 2?
==============

state in query_or_respond:  {'messages': [HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='4510445e-ed6a-47b2-836b-b2f4d664d3c7')]} [SystemMessage(content='Use the retrieve tool if external information is required.', additional_kwargs={}, response_metadata={}), HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='4510445e-ed6a-47b2-836b-b2f4d664d3c7')]
Response llm:  content='' additional_kwargs={'tool_calls': [{'id': 'zpxkfbb56', 'function': {'arguments': '{"query":"table 2"}', 'name': 'retrieve'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 351, 'prompt_tokens': 161, 'total_tokens': 512, 'completion_time': 0.72634568, 'completion_tokens_details': {'reasoning_tokens': 327}, 'prompt_time': 0.007195902, 'prompt_tokens_details': None, 'queue_time': 0.159124408, 'total_time': 0.733541582}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b9299-7988-7873-8387-e8bfddbbbd70-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'table 2'}, 'id': 'zpxkfbb56', 'type': 'tool_call'}] usage_metadata={'input_tokens': 161, 'output_tokens': 351, 'total_tokens': 512, 'output_token_details': {'reasoning': 327}}
[HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='4510445e-ed6a-47b2-836b-b2f4d664d3c7'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'zpxkfbb56', 'function': {'arguments': '{"query":"table 2"}', 'name': 'retrieve'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 351, 'prompt_tokens': 161, 'total_tokens': 512, 'completion_time': 0.72634568, 'completion_tokens_details': {'reasoning_tokens': 327}, 'prompt_time': 0.007195902, 'prompt_tokens_details': None, 'queue_time': 0.159124408, 'total_time': 0.733541582}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b9299-7988-7873-8387-e8bfddbbbd70-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'table 2'}, 'id': 'zpxkfbb56', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 351, 'total_tokens': 512, 'output_token_details': {'reasoning': 327}})] describe  table 2?
route decision:  RETRIEVE
idx: 0 => subq: 1. What are the columns or variables included in Table 2? ==> qa_pairs: []
idx: 1 => subq: 2. What data types or categories are associated with each column in Table 2? ==> qa_pairs: ['Question: describe  table 2?\nAnswer: content=\'The columns or variables in Table 2 cannot be explicitly determined from the provided context because the table itself is not visible. However, based on the surrounding text and the data lines (e.g., model names followed by numerical values), the following inferences can be made:\\n\\n1. **Model Names/Identifiers**: The first column likely lists the names or identifiers of different models (e.g., "Dolphin[11]", "Marker[1]", "Nougat[6]", etc.), possibly with version numbers or citations in brackets.\\n\\n2. **Performance Metrics**: The subsequent columns likely represent numerical performance metrics (e.g., precision, recall, F1 score, or other evaluation scores) across different benchmarks or document categories. For example:\\n   - Decimal values (e.g., 0.356, 0.465) might correspond to metrics like accuracy, precision, or recall for specific tasks.\\n   - The text mentions "decoding precision" in relation to compression ratios, suggesting at least one column could represent precision under specific conditions.\\n\\n3. **Compression Ratios**: The context discusses compression ratios (e.g., "10× compression ratio"), which might be a separate column or implicitly tied to the metrics (e.g., metrics evaluated at varying compression levels).\\n\\n4. **Document Categories**: The mention of "Edit distances for different categories of documents" in Table 4 (e.g., "Book," "Slides," "Financial Report") hints that Table 2 might also include similar categories as columns, though this is speculative.\\n\\n**Final Answer**:  \\nThe exact columns of Table 2 are not explicitly provided in the context. However, based on the data and text, it likely includes **model identifiers** followed by **numerical performance metrics** (e.g., precision, recall, or F1 scores) across different benchmarks, document types, or compression conditions. The text specifically references "decoding precision" and compression ratios, suggesting these variables may be part of the table.\' additional_kwargs={} response_metadata={\'token_usage\': {\'completion_tokens\': 1569, \'prompt_tokens\': 2619, \'total_tokens\': 4188, \'completion_time\': 3.335597408, \'completion_tokens_details\': {\'reasoning_tokens\': 1158}, \'prompt_time\': 0.122192511, \'prompt_tokens_details\': None, \'queue_time\': 0.051634829, \'total_time\': 3.457789919}, \'model_name\': \'qwen/qwen3-32b\', \'system_fingerprint\': \'fp_5cf921caa2\', \'service_tier\': \'on_demand\', \'finish_reason\': \'stop\', \'logprobs\': None, \'model_provider\': \'groq\'} id=\'lc_run--019b9299-86b2-7a91-80df-738265914fd5-0\' usage_metadata={\'input_tokens\': 2619, \'output_tokens\': 1569, \'total_tokens\': 4188, \'output_token_details\': {\'reasoning\': 1158}}\n\n']
idx: 2 => subq: 3. What summary statistics or key findings are presented in Table 2? ==> qa_pairs: ['Question: describe  table 2?\nAnswer: content=\'The columns or variables in Table 2 cannot be explicitly determined from the provided context because the table itself is not visible. However, based on the surrounding text and the data lines (e.g., model names followed by numerical values), the following inferences can be made:\\n\\n1. **Model Names/Identifiers**: The first column likely lists the names or identifiers of different models (e.g., "Dolphin[11]", "Marker[1]", "Nougat[6]", etc.), possibly with version numbers or citations in brackets.\\n\\n2. **Performance Metrics**: The subsequent columns likely represent numerical performance metrics (e.g., precision, recall, F1 score, or other evaluation scores) across different benchmarks or document categories. For example:\\n   - Decimal values (e.g., 0.356, 0.465) might correspond to metrics like accuracy, precision, or recall for specific tasks.\\n   - The text mentions "decoding precision" in relation to compression ratios, suggesting at least one column could represent precision under specific conditions.\\n\\n3. **Compression Ratios**: The context discusses compression ratios (e.g., "10× compression ratio"), which might be a separate column or implicitly tied to the metrics (e.g., metrics evaluated at varying compression levels).\\n\\n4. **Document Categories**: The mention of "Edit distances for different categories of documents" in Table 4 (e.g., "Book," "Slides," "Financial Report") hints that Table 2 might also include similar categories as columns, though this is speculative.\\n\\n**Final Answer**:  \\nThe exact columns of Table 2 are not explicitly provided in the context. However, based on the data and text, it likely includes **model identifiers** followed by **numerical performance metrics** (e.g., precision, recall, or F1 scores) across different benchmarks, document types, or compression conditions. The text specifically references "decoding precision" and compression ratios, suggesting these variables may be part of the table.\' additional_kwargs={} response_metadata={\'token_usage\': {\'completion_tokens\': 1569, \'prompt_tokens\': 2619, \'total_tokens\': 4188, \'completion_time\': 3.335597408, \'completion_tokens_details\': {\'reasoning_tokens\': 1158}, \'prompt_time\': 0.122192511, \'prompt_tokens_details\': None, \'queue_time\': 0.051634829, \'total_time\': 3.457789919}, \'model_name\': \'qwen/qwen3-32b\', \'system_fingerprint\': \'fp_5cf921caa2\', \'service_tier\': \'on_demand\', \'finish_reason\': \'stop\', \'logprobs\': None, \'model_provider\': \'groq\'} id=\'lc_run--019b9299-86b2-7a91-80df-738265914fd5-0\' usage_metadata={\'input_tokens\': 2619, \'output_tokens\': 1569, \'total_tokens\': 4188, \'output_token_details\': {\'reasoning\': 1158}}\n\n', 'Question: describe  table 2?\nAnswer: content=\'The data types or categories associated with each column in Table 2 can be inferred as follows based on the provided context and data examples:\\n\\n1. **Model Identifiers/Names**:  \\n   - The first column lists model names or identifiers (e.g., "Dolphin[11]", "Marker[1]", "Nougat[6]"), often with citations or version numbers in brackets. These likely represent different OCR or document processing models.\\n\\n2. **Numerical Performance Metrics**:  \\n   - Subsequent columns contain decimal values (e.g., 0.356, 0.465, 0.258) representing **evaluation metrics** such as precision, recall, F1 scores, or accuracy. These metrics likely correspond to performance across specific tasks or benchmarks (e.g., chart parsing, formula recognition, or HTML table conversion).\\n\\n3. **Compression Ratios or Model Parameters**:  \\n   - Some entries include integers (e.g., "2352", "392", "6790") or hyphens (e.g., "-") in early columns. These could indicate **compression ratios** (e.g., "10× compression") or model-specific parameters (e.g., parameter counts or token limits), though this is less certain.\\n\\n4. **Document Categories or Task-Specific Metrics**:  \\n   - The decimal values may correspond to performance across **document categories** (e.g., charts, formulas, tables) or **task-specific benchmarks** (e.g., "decoding precision" mentioned in the context). For example, values might reflect scores for "chart parsing," "formula recognition," or "HTML table conversion."\\n\\n5. **End-to-End Model Performance**:  \\n   - For end-to-end models (e.g., "Nougat[6]", "GOT-OCR2.0"), additional numerical values (e.g., "2352", "3949") might represent **token counts**, **processing speeds**, or **compression ratios**, though this requires further clarification.\\n\\n---\\n\\n### Key Notes:\\n- The exact column headers are not explicitly provided in the context, but the data patterns suggest a structure where **model names** are followed by **numerical performance metrics** across tasks or conditions.\\n- Metrics like "decoding precision" and mentions of compression ratios imply some columns are tied to specific evaluation conditions.\\n- The decimal values (e.g., 0.356, 0.465) align with standard evaluation metrics (precision, recall, F1 scores) used in OCR and document analysis tasks.\' additional_kwargs={} response_metadata={\'token_usage\': {\'completion_tokens\': 1230, \'prompt_tokens\': 3117, \'total_tokens\': 4347, \'completion_time\': 2.6573002150000002, \'completion_tokens_details\': {\'reasoning_tokens\': 693}, \'prompt_time\': 0.140924348, \'prompt_tokens_details\': None, \'queue_time\': 0.159701741, \'total_time\': 2.7982245629999998}, \'model_name\': \'qwen/qwen3-32b\', \'system_fingerprint\': \'fp_2bfcc54d36\', \'service_tier\': \'on_demand\', \'finish_reason\': \'stop\', \'logprobs\': None, \'model_provider\': \'groq\'} id=\'lc_run--019b9299-9719-7c42-abe1-608dce16e5cc-0\' usage_metadata={\'input_tokens\': 3117, \'output_tokens\': 1230, \'total_tokens\': 4347, \'output_token_details\': {\'reasoning\': 693}}\n\n']
{'messages': [HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='4510445e-ed6a-47b2-836b-b2f4d664d3c7'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'zpxkfbb56', 'function': {'arguments': '{"query":"table 2"}', 'name': 'retrieve'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 351, 'prompt_tokens': 161, 'total_tokens': 512, 'completion_time': 0.72634568, 'completion_tokens_details': {'reasoning_tokens': 327}, 'prompt_time': 0.007195902, 'prompt_tokens_details': None, 'queue_time': 0.159124408, 'total_time': 0.733541582}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b9299-7988-7873-8387-e8bfddbbbd70-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'table 2'}, 'id': 'zpxkfbb56', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 351, 'total_tokens': 512, 'output_token_details': {'reasoning': 327}})], 'sub_questions': ['1. What are the columns or variables included in Table 2?', '2. What data types or categories are associated with each column in Table 2?', '3. What summary statistics or key findings are presented in Table 2?'], 'current_subq_index': 3, 'qa_pairs': ['Question: describe  table 2?\nAnswer: content=\'The columns or variables in Table 2 cannot be explicitly determined from the provided context because the table itself is not visible. However, based on the surrounding text and the data lines (e.g., model names followed by numerical values), the following inferences can be made:\\n\\n1. **Model Names/Identifiers**: The first column likely lists the names or identifiers of different models (e.g., "Dolphin[11]", "Marker[1]", "Nougat[6]", etc.), possibly with version numbers or citations in brackets.\\n\\n2. **Performance Metrics**: The subsequent columns likely represent numerical performance metrics (e.g., precision, recall, F1 score, or other evaluation scores) across different benchmarks or document categories. For example:\\n   - Decimal values (e.g., 0.356, 0.465) might correspond to metrics like accuracy, precision, or recall for specific tasks.\\n   - The text mentions "decoding precision" in relation to compression ratios, suggesting at least one column could represent precision under specific conditions.\\n\\n3. **Compression Ratios**: The context discusses compression ratios (e.g., "10× compression ratio"), which might be a separate column or implicitly tied to the metrics (e.g., metrics evaluated at varying compression levels).\\n\\n4. **Document Categories**: The mention of "Edit distances for different categories of documents" in Table 4 (e.g., "Book," "Slides," "Financial Report") hints that Table 2 might also include similar categories as columns, though this is speculative.\\n\\n**Final Answer**:  \\nThe exact columns of Table 2 are not explicitly provided in the context. However, based on the data and text, it likely includes **model identifiers** followed by **numerical performance metrics** (e.g., precision, recall, or F1 scores) across different benchmarks, document types, or compression conditions. The text specifically references "decoding precision" and compression ratios, suggesting these variables may be part of the table.\' additional_kwargs={} response_metadata={\'token_usage\': {\'completion_tokens\': 1569, \'prompt_tokens\': 2619, \'total_tokens\': 4188, \'completion_time\': 3.335597408, \'completion_tokens_details\': {\'reasoning_tokens\': 1158}, \'prompt_time\': 0.122192511, \'prompt_tokens_details\': None, \'queue_time\': 0.051634829, \'total_time\': 3.457789919}, \'model_name\': \'qwen/qwen3-32b\', \'system_fingerprint\': \'fp_5cf921caa2\', \'service_tier\': \'on_demand\', \'finish_reason\': \'stop\', \'logprobs\': None, \'model_provider\': \'groq\'} id=\'lc_run--019b9299-86b2-7a91-80df-738265914fd5-0\' usage_metadata={\'input_tokens\': 2619, \'output_tokens\': 1569, \'total_tokens\': 4188, \'output_token_details\': {\'reasoning\': 1158}}\n\n', 'Question: describe  table 2?\nAnswer: content=\'The data types or categories associated with each column in Table 2 can be inferred as follows based on the provided context and data examples:\\n\\n1. **Model Identifiers/Names**:  \\n   - The first column lists model names or identifiers (e.g., "Dolphin[11]", "Marker[1]", "Nougat[6]"), often with citations or version numbers in brackets. These likely represent different OCR or document processing models.\\n\\n2. **Numerical Performance Metrics**:  \\n   - Subsequent columns contain decimal values (e.g., 0.356, 0.465, 0.258) representing **evaluation metrics** such as precision, recall, F1 scores, or accuracy. These metrics likely correspond to performance across specific tasks or benchmarks (e.g., chart parsing, formula recognition, or HTML table conversion).\\n\\n3. **Compression Ratios or Model Parameters**:  \\n   - Some entries include integers (e.g., "2352", "392", "6790") or hyphens (e.g., "-") in early columns. These could indicate **compression ratios** (e.g., "10× compression") or model-specific parameters (e.g., parameter counts or token limits), though this is less certain.\\n\\n4. **Document Categories or Task-Specific Metrics**:  \\n   - The decimal values may correspond to performance across **document categories** (e.g., charts, formulas, tables) or **task-specific benchmarks** (e.g., "decoding precision" mentioned in the context). For example, values might reflect scores for "chart parsing," "formula recognition," or "HTML table conversion."\\n\\n5. **End-to-End Model Performance**:  \\n   - For end-to-end models (e.g., "Nougat[6]", "GOT-OCR2.0"), additional numerical values (e.g., "2352", "3949") might represent **token counts**, **processing speeds**, or **compression ratios**, though this requires further clarification.\\n\\n---\\n\\n### Key Notes:\\n- The exact column headers are not explicitly provided in the context, but the data patterns suggest a structure where **model names** are followed by **numerical performance metrics** across tasks or conditions.\\n- Metrics like "decoding precision" and mentions of compression ratios imply some columns are tied to specific evaluation conditions.\\n- The decimal values (e.g., 0.356, 0.465) align with standard evaluation metrics (precision, recall, F1 scores) used in OCR and document analysis tasks.\' additional_kwargs={} response_metadata={\'token_usage\': {\'completion_tokens\': 1230, \'prompt_tokens\': 3117, \'total_tokens\': 4347, \'completion_time\': 2.6573002150000002, \'completion_tokens_details\': {\'reasoning_tokens\': 693}, \'prompt_time\': 0.140924348, \'prompt_tokens_details\': None, \'queue_time\': 0.159701741, \'total_time\': 2.7982245629999998}, \'model_name\': \'qwen/qwen3-32b\', \'system_fingerprint\': \'fp_2bfcc54d36\', \'service_tier\': \'on_demand\', \'finish_reason\': \'stop\', \'logprobs\': None, \'model_provider\': \'groq\'} id=\'lc_run--019b9299-9719-7c42-abe1-608dce16e5cc-0\' usage_metadata={\'input_tokens\': 3117, \'output_tokens\': 1230, \'total_tokens\': 4347, \'output_token_details\': {\'reasoning\': 693}}\n\n', 'Question: describe  table 2?\nAnswer: content=\'The summary statistics and key findings presented in Table 2, based on the provided context and inferred structure, include the following:\\n\\n1. **Decoding Precision at Compression Ratios**:  \\n   - At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance under moderate compression.  \\n   - Beyond 10× compression, performance declines due to factors like increased layout complexity in long documents and reduced text clarity at lower resolutions (e.g., 512×512 or 640×640).  \\n   - At **20× compression**, precision remains at **~60%**, suggesting some resilience to extreme compression.\\n\\n2. **Model Performance Metrics**:  \\n   - Numerical values (e.g., 0.356, 0.465, 0.258) likely represent **task-specific metrics** (precision, recall, or F1 scores) across document categories (e.g., charts, formulas, tables) or benchmarks.  \\n   - End-to-end models (e.g., **Nougat**, **SmolDocling**) show higher scores in specific categories (e.g., 0.973 for "Book" in Nougat), while others struggle with tasks like "Financial Report" or "Exam Paper."\\n\\n3. **Compression Trade-offs**:  \\n   - The study highlights that optical context compression is promising, with minimal overhead since it leverages existing vision-language model (VLM) infrastructure.  \\n   - Performance degradation at higher compression ratios is attributed to layout complexity and text blurring, suggesting opportunities for improvements in rendering techniques.\\n\\n4. **Comparative Model Analysis**:  \\n   - Models like **Dolphin[11]** and **Nougat[6]** outperform others in specific categories (e.g., Dolphin[11] scores 0.44 in "Academic Papers"), while models like **GOT-OCR2.0** and **OCRFlux-3B** show lower overall performance.  \\n   - Token counts (e.g., "2352," "392") for end-to-end models may correlate with processing speeds or parameter efficiency, though exact interpretations require further clarification.\\n\\n### Key Takeaways:\\n- **High precision at 10× compression** validates the effectiveness of optical context compression.  \\n- **Performance trends** (decline beyond 10×) highlight technical challenges in maintaining quality at extreme compression levels.  \\n- The table emphasizes the **variability of model performance** across document types, underscoring the need for task-specific optimizations.  \\n\\nThese findings position optical context compression as a promising research direction with practical applications in document processing.\' additional_kwargs={} response_metadata={\'token_usage\': {\'completion_tokens\': 1032, \'prompt_tokens\': 4294, \'total_tokens\': 5326, \'completion_time\': 2.582913366, \'completion_tokens_details\': {\'reasoning_tokens\': 469}, \'prompt_time\': 0.190311872, \'prompt_tokens_details\': None, \'queue_time\': 0.058009557, \'total_time\': 2.773225238}, \'model_name\': \'qwen/qwen3-32b\', \'system_fingerprint\': \'fp_5cf921caa2\', \'service_tier\': \'on_demand\', \'finish_reason\': \'stop\', \'logprobs\': None, \'model_provider\': \'groq\'} id=\'lc_run--019b9299-f199-7051-baaf-0f9a01c3a9eb-0\' usage_metadata={\'input_tokens\': 4294, \'output_tokens\': 1032, \'total_tokens\': 5326, \'output_token_details\': {\'reasoning\': 469}}\n\n'], 'retrieved_context': 'Large 400(285) 0.138 0.054 0.277 0.152 0.067 0.208 0.143 0.461 0.104 0.123\nGundam 795 0.127 0.043 0.269 0.134 0.062 0.181 0.097 0.432 0.089 0.103\nGundam-M†200dpi 18530.1230.0490.2420.147 0.0560.1570.0870.377 0.080.085\nwithout layout: "<image>\\nFree OCR."to control the model’s output format.Nevertheless, the\noutput format still cannot completely match Fox benchmarks, so the actual performance would\nbe somewhat higher than the test results.As shown in Table 2, within a 10×compression ratio, the model’s decoding precision can\nreach approximately 97%, which is a very promising result.In the future, it may be possible to\nachieve nearly 10×lossless contexts compression through text-to-image approaches.When the\ncompression ratio exceeds 10×, performance begins to decline, which may have two reasons:\none is that the layout of long documents becomes more complex, and another reason may be\nthat long texts become blurred at 512×512 or 640×640 resolution.The first issue can be solved\nby rendering texts onto a single layout page, while we believe the second issue will become\n11 a feature of the forgetting mechanism.When compressing tokens by nearly 20×, we find that\nprecision can still approach 60%.These results indicate that optical contexts compression is\na very promising and worthwhile research direction, and this approach does not bring any\noverhead because it can leverage VLM infrastructure, as multimodal systems inherently require\nan additional vision encoder.Table 4 |Edit distances for different categories of documents in OmniDocBench.The results\nshow that some types of documents can achieve good performance with just 64 or 100 vision\ntokens, while others require Gundam mode.Mode\nType Book Slides Financial\nReport Textbook Exam\nPaper Magazine Academic\nPapers Notes Newspaper Overall\nTiny 0.147 0.116 0.207 0.173 0.294 0.201 0.395 0.297 0.94 0.32\nSmall 0.085 0.111 0.079 0.147 0.171 0.107 0.131 0.187 0.744 0.205\nBase 0.037 0.08 0.027 0.1 0.13 0.073 0.052 0.176 0.645 0.156\nDolphin[11] - 0.356 0.352 0.465 0.258 0.35 0.44 0.44 0.604 0.367 0.351\nMarker[1] - 0.296 0.085 0.374 0.609 0.116 0.497 0.293 0.688 0.678 0.329\nMathpix[2] - 0.191 0.105 0.306 0.243 0.108 0.364 0.381 0.454 0.32 0.30\nMinerU-2.1.1[34] - 0.162 0.072 0.313 0.166 0.097 0.244 0.111 0.581 0.15 0.136\nMonkeyOCR-1.2B[18] - 0.154 0.062 0.295 0.164 0.094 0.263 0.179 0.464 0.168 0.243\nPPstructure-v3 [9] - 0.152 0.073 0.295 0.162 0.077 0.223 0.136 0.535 0.111 0.11\nEnd-to-end Models\nNougat[6] 2352 0.452 0.365 0.488 0.572 0.382 0.973 0.998 0.941 1.00 0.954\nSmolDocling [25] 392 0.493 0.262 0.753 0.729 0.227 0.816 0.838 0.997 0.907 0.522\nInternVL2-76B[8] 6790 0.44 0.353 0.543 0.547 0.317 0.443 0.29 0.701 0.555 0.228\nQwen2.5-VL-7B[5] 3949 0.316 0.151 0.376 0.598 0.138 0.399 0.243 0.5 0.627 0.226\nOLMOCR[28] 3949 0.326 0.097 0.455 0.608 0.145 0.469 0.293 0.655 0.652 0.277\nGOT-OCR2.0 [38] 256 0.287 0.189 0.360 0.459 0.141 0.411 0.315 0.528 0.52 0.28\nOCRFlux-3B[3] 3949 0.238 0.112 0.447 0.269 0.126 0.349 0.256 0.716 0.162 0.263\nGPT4o[26] - 0.233 0.144 0.425 0.234 0.128 0.399 0.409 0.606 0.329 0.251\nInternVL3-78B [42] 6790 0.218 0.117 0.38 0.279 0.095 0.296 0.21 0.533 0.282 0.161\nQwen2.5-VL-72B[5] 3949 0.214 0.092 0.315 0.341 0.106 0.261 0.18 0.434 0.262 0.168\ndots.ocr[30] 3949 0.182 0.137 0.320 0.166 0.182 0.261 0.229 0.468 0.160 0.261\nGemini2.5-Pro [4] - 0.148 0.055 0.356 0.13 0.049 0.212 0.168 0.439 0.119 0.121\nMinerU2.0[34] 6790 0.133 0.045 0.273 0.15 0.066 0.238 0.115 0.506 0.209 0.122\ndots.ocr†200dpi[30] 5545 0.1250.0320.3290.099 0.040.160.0660.416 0.0920.067\nDeepSeek-OCR (end2end)\nTiny640.386 0.373 0.469 0.422 0.283 0.361 0.307 0.635 0.266 0.236\nSmall 100 0.221 0.142 0.373 0.242 0.125 0.284 0.24 0.53 0.159 0.205\nBase 256(182) 0.137 0.054 0.267 0.163 0.064 0.24 0.205 0.474 0.1 0.181\nLarge 400(285) 0.138 0.054 0.277 0.152 0.067 0.208 0.143 0.461 0.104 0.123\nGundam 795 0.127 0.043 0.269 0.134 0.062 0.181 0.097 0.432 0.089 0.103'}