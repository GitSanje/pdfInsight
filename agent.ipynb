{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad29ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #https://github.com/Future-House/paper-qa.git\n",
    "# from google.genai import types\n",
    "# from google import genai\n",
    "\n",
    "# # # Only run this block for Vertex AI API\n",
    "# client = genai.Client(api_key=\"AIzaSyDn82tmjyQRkZr3K79CXWC47ab3Xko29L0\")\n",
    "\n",
    "# !pip install py-spy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Get a single environment variable\n",
    "API_KEY_GROQ = os.getenv(\"API_KEY_GROQ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01863a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-community arxiv pymupdf langchain pypdf\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install -U transformers\n",
    "# !pip install -qU langchain_google_genai\n",
    "# !pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4e57d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanje/.pyenv/versions/3.12.3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "def get_text_from_pdf(file_path=None, url=None):\n",
    "    if url:\n",
    "        response = requests.get(url)\n",
    "        pdf_bytes = response.content\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_pdf:\n",
    "            temp_pdf.write(pdf_bytes)\n",
    "            file_path = temp_pdf.name\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split()\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_text_from_pdf(url=\"https://arxiv.org/pdf/2510.18234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd73534",
   "metadata": {},
   "source": [
    "## Chunking techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9441fa",
   "metadata": {},
   "source": [
    "### 1. Sliding Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabffaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [ doc.page_content for doc in docs ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998d6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text:str) :\n",
    "\n",
    "    cleaned_text = re.sub(r\"(Contents).*?1\\. Introduction\", r\"\\1\\n\", text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r\"(References|REFERENCES).*?$\", \"\", cleaned_text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96110956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text = clean_text(' '.join(pages))\n",
    "# display(Markdown(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d13784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "MAX_TEXT_ENTROPY = 8.0\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def maybe_is_text(s: str, thresh: float = 2.5) -> bool:\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the string to discard files with excessively repeated symbols.\n",
    "\n",
    "    PDF parsing sometimes represents horizontal distances between words on title pages\n",
    "    and in tables with spaces, which should therefore not be included in this calculation.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    s_wo_spaces = s.replace(\" \", \"\")\n",
    "    if not s_wo_spaces:\n",
    "        return False\n",
    "\n",
    "    counts = Counter(s_wo_spaces)\n",
    "    entropy = 0.0\n",
    "    length = len(s_wo_spaces)\n",
    "    for count in counts.values():\n",
    "        p = count / length\n",
    "        entropy += -p * math.log2(p)\n",
    "\n",
    "    # Check if the entropy is within a reasonable range for text\n",
    "    return MAX_TEXT_ENTROPY > entropy > thresh\n",
    "\n",
    "\n",
    "sentences = split_sentences(clean_text(' '.join(pages)))\n",
    "text_sentences = [s for s in sentences if maybe_is_text(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a34528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/gemini-embedding-001\",\n",
    "             google_api_key=\"AIzaSyDEckSvtc3k_d0KgXyPgsvC1nUUjYc7xBk\",\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=150,\n",
    "        )\n",
    "\n",
    "text = ''.join(text_sentences)\n",
    "docs = splitter.create_documents([text])\n",
    "print(len(docs))\n",
    "class MemoryStore():\n",
    "    def __init__(self, embeddings):\n",
    "        self.store = InMemoryVectorStore(embedding=embeddings)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "      \n",
    "        return  self.store.add_documents(documents)\n",
    "\n",
    "    def similarity_search(self, query, k=4):\n",
    "        return self.store.similarity_search(query, k)\n",
    "    \n",
    "\n",
    "memory_store = MemoryStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47da6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 24 documents to memory store\n"
     ]
    }
   ],
   "source": [
    "ids = memory_store.add_documents(docs)\n",
    "print(f\"Added {len(ids)} documents to memory store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fba2baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state(state, node_name):\n",
    "    print(f\"\\n=== {node_name} ===\")\n",
    "    for i, m in enumerate(state[\"messages\"]):\n",
    "        print(f\"[{i}] {m.__class__.__name__}: {m.content}\")\n",
    "        if getattr(m, \"tool_calls\", None):\n",
    "            print(\"   tool_calls:\", m.tool_calls)\n",
    "    print(\"==============\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd69211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class LLMProvider:\n",
    "    def __init__(self, api_key: str):\n",
    "        # self.llm = ChatGoogleGenerativeAI(\n",
    "        #     model=\"gemini-2.5-flash\",\n",
    "        #     temperature=0,\n",
    "        #     api_key=api_key,\n",
    "        # )\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"qwen/qwen3-32b\",\n",
    "            temperature=0,\n",
    "            reasoning_format=\"hidden\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "    def base(self):\n",
    "        return self.llm\n",
    "    \n",
    "    def with_tools(self, tools):\n",
    "        return self.llm.bind_tools(tools, tool_choice=\"retrieve\") \n",
    "    \n",
    "\n",
    "class RetrievalTool:\n",
    "    def __init__(self, vectorstore, llm_provider:LLMProvider):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm_provider = llm_provider\n",
    "        self.tool = self._build_tool()\n",
    "        self.step_back_tool = self._step_back_build_tool()\n",
    "        self.MULTI_QUERY_PROMPT =  \"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "                    different versions of the given user question to retrieve relevant documents from a vector \n",
    "                    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "                    the user overcome some of the limitations of the distance-based similarity search. \n",
    "                    Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "\n",
    "    def get_unique_union(self, results):\n",
    "        \"\"\" Unique union of retrieved docs \"\"\"\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for docs in results:\n",
    "            for doc in docs: \n",
    "                if isinstance(doc, tuple):\n",
    "                   doc = doc[0]\n",
    "                if doc.page_content not in seen:\n",
    "                    seen.add(doc.page_content)\n",
    "                unique_docs.append(doc.page_content)\n",
    "        return unique_docs\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def reciprocal_rank_fusion(self, results: list[list], k=60):\n",
    "        \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "            and an optional parameter k used in the RRF formula \"\"\"\n",
    "        # Initialize a dictionary to hold fused scores for each unique document\n",
    "        fused_scores = {}\n",
    "        for docs in results:\n",
    "            print(f\"len of docs : {len(docs)}\")\n",
    "            for rank, doc in enumerate(docs):\n",
    "                doc_str= doc.page_content\n",
    "                # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "                if doc_str not in fused_scores:\n",
    "                    fused_scores[doc_str] = 0\n",
    "                # Retrieve the current score of the document, if any\n",
    "                previous_score = fused_scores[doc_str]\n",
    "                # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "                fused_scores[doc_str] += 1 / (rank + k)\n",
    "        # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "        reranked_results = [\n",
    "         (  doc,score )for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "        for doc , score in reranked_results:\n",
    "            print ( \"==================\")\n",
    "            print( f\" Score: {score}  === DOC: {doc}\")\n",
    "            print ( \"==================\")\n",
    "        # print(len(reranked_results))\n",
    "        return reranked_results\n",
    "\n",
    "   \n",
    "  \n",
    "    async def retrieve(self,query: str):\n",
    "    \n",
    "        docs = await asyncio.to_thread(\n",
    "                    self.vectorstore.similarity_search, query, 2\n",
    "                )\n",
    "        serialized = \"\\n\".join(d.page_content for d in docs)\n",
    "        return serialized\n",
    "    \n",
    "    def _step_back_build_tool(self):\n",
    "        @tool(description=\"Retrieve documents using original and step-back query.\")\n",
    "        async def retrieve(state: dict):\n",
    "            # Extract step-back question from state\n",
    "            step_back_q = state.get(\"step_back_question\", None)\n",
    "            if not step_back_q:\n",
    "                # fallback to original question\n",
    "                step_back_q = state[\"messages\"][0].content\n",
    "            \n",
    "            original_q = state[\"messages\"][0].content\n",
    "\n",
    "            step_back_docs = await asyncio.to_thread(\n",
    "                 self.vectorstore.similarity_search, step_back_q, k=3\n",
    "            )\n",
    "            original_q_docs = await asyncio.to_thread(\n",
    "                 self.vectorstore.similarity_search, original_q, k=3\n",
    "            )\n",
    "\n",
    "            serialized1 = \"\\n\".join(d.page_content for d in step_back_docs)\n",
    "            serialized2 = \"\\n\".join(d.page_content for d in original_q_docs)\n",
    "\n",
    "            return serialized1+\"\\n\"+serialized2\n",
    "\n",
    "        return retrieve\n",
    "    \n",
    "\n",
    "    def _build_tool(self):\n",
    "        \n",
    "        @tool(description=\"Retrieve documents using multi-query expansion.\")\n",
    "        async def retrieve(query: str):\n",
    "\n",
    "             # 1️⃣ Generate multiple queries\n",
    "\n",
    "            expansion_response = await self.llm_provider.base().ainvoke(\n",
    "                    self.MULTI_QUERY_PROMPT.format(question=query)\n",
    "                )\n",
    "\n",
    "            print(\"expansion_response: \",expansion_response)\n",
    "            queries = [\n",
    "                q.strip() for q in expansion_response.content.split(\"\\n\")\n",
    "                if q.strip()\n",
    "            ]\n",
    "            queries.append(query)\n",
    "\n",
    "             # 2️⃣ Retrieve docs for each query\n",
    "            all_docs = []\n",
    "            for q in queries:\n",
    "                #similarity_search() is blocking CPU-bound code\n",
    "                #Runs the function in a thread pool\n",
    "                #Frees the event loop\n",
    "                \n",
    "                docs = await asyncio.to_thread(\n",
    "                    self.vectorstore.similarity_search, q, 2\n",
    "                )\n",
    "                print(f\"Retrieved {docs} documents for query: {q}\")\n",
    "                all_docs.append(docs)\n",
    " \n",
    "\n",
    "            fused_docs = self.reciprocal_rank_fusion(all_docs)\n",
    "            \n",
    "            serialized = \"\\n\".join([doc[0] for doc in fused_docs[:3]])\n",
    "            # union_docs = self.get_unique_union(all_docs)\n",
    "            # serialized = \"\\n\".join(union_docs[:3])\n",
    "            print(\"serialized: \",serialized)\n",
    "            return serialized\n",
    "\n",
    "        return retrieve\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73ea003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph.message import  add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage, AIMessage, ToolMessage, HumanMessage\n",
    ")\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    # Conversation\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "    # Decomposition\n",
    "    sub_questions: List[str]\n",
    "    current_subq_index: int\n",
    "\n",
    "    # Memory across sub-questions\n",
    "    qa_pairs: List[str]\n",
    "\n",
    "    retrieved_context:str\n",
    "\n",
    "    #step_back question\n",
    "    step_back_question:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c993f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "ROUTER_PROMPT = \"\"\"\n",
    "You are a router for a question-answering system.\n",
    "\n",
    "Decide whether the user's question requires looking up\n",
    "information from documents or can be answered directly.\n",
    "\n",
    "Answer with ONLY one word:\n",
    "RETRIEVE or DIRECT\n",
    "\n",
    "Question:\n",
    "\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "class RAGNodes:\n",
    "    def __init__(self, llm_provider : LLMProvider, retrieve_tool:RetrievalTool):\n",
    "        self.llm_provider = llm_provider\n",
    "        self.retrieve_tool = retrieve_tool\n",
    "\n",
    "    async def query_or_respond(self, state: GraphState):\n",
    "        \n",
    "        llm = self.llm_provider.with_tools(\n",
    "            [self.retrieve_tool.tool]\n",
    "          \n",
    "        )\n",
    "\n",
    "        messages = list(state[\"messages\"])\n",
    "        if not any(isinstance(m, SystemMessage) for m in messages):\n",
    "            messages.insert(0, SystemMessage(\n",
    "                \"Use the retrieve tool if external information is required.\"\n",
    "            ))\n",
    "        print_state(state, \"query_or_respond\")\n",
    "        print( \"state in query_or_respond: \",state , messages)\n",
    "        response =  await llm.ainvoke(messages)\n",
    "        print(\"Response llm: \", response)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    async def route(self, state: GraphState):\n",
    "        question = state[\"messages\"][0].content\n",
    "        print(state['messages'], question)\n",
    "        response = await self.llm_provider.base().ainvoke(\n",
    "        ROUTER_PROMPT.format(question=question)\n",
    "         )\n",
    "        \n",
    "        print(\"route decision: \",response.content)\n",
    "        decision = response.content.strip().upper()\n",
    "        if decision not in {\"RETRIEVE\", \"DIRECT\"}:\n",
    "            decision = \"RETRIEVE\"  # safe fallback\n",
    "        if decision == \"RETRIEVE\":\n",
    "            return \"stepBackq\"\n",
    "        return\"generate\"\n",
    "    \n",
    "    async def decompose(self, state: GraphState):\n",
    "            question = state[\"messages\"][0].content\n",
    "            # Decomposition\n",
    "            template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "            The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "            Generate multiple search queries related to: {question} \\n\n",
    "            Output (3 queries):\"\"\"\n",
    "\n",
    "            response =  await self.llm_provider.base().ainvoke(\n",
    "                template.format(question= question)\n",
    "            )\n",
    "            subqs = [q.strip() for q in response.content.split(\"\\n\") if q.strip()]\n",
    "\n",
    "\n",
    "            return  {\n",
    "            \"sub_questions\": subqs,\n",
    "            \"current_subq_index\": 0,\n",
    "            \"qa_pairs\": []\n",
    "            }\n",
    "\n",
    "\n",
    "    async def step_back_retrieve(self, state: GraphState):\n",
    "        original_q = state[\"messages\"][0].content\n",
    "        step_back_q = state.get(\"step_back_question\", original_q)\n",
    "\n",
    "        docs1 = await self.retrieve_tool.retrieve(original_q)\n",
    "        docs2 = await self.retrieve_tool.retrieve(step_back_q)\n",
    "        print(f\"[step_back_retrieve] Original: {docs1}\")\n",
    "        print(f\"[step_back_retrieve] Step-back: {docs2}\")\n",
    "        \n",
    "        context =  docs1 +\"\\n\"+ docs2\n",
    "\n",
    "        return {\n",
    "            \"retrieved_context\": context\n",
    "        }\n",
    "    async def step_back__q_generate(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Generates a step-back version of the question using few-shot examples.\n",
    "        Stores it in the state for retrieval.\n",
    "        \"\"\"\n",
    "         \n",
    "         # Step-back few-shot examples\n",
    "        examples = [\n",
    "            {\n",
    "                \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "                \"output\": \"what can the members of The Police do?\",\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "                \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "            },\n",
    "        ]\n",
    "        example_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"human\", \"{input}\"),\n",
    "                (\"ai\", \"{output}\"),\n",
    "            ]\n",
    "        )\n",
    "        few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=example_prompt,\n",
    "            examples=examples,\n",
    "        )\n",
    "\n",
    "        step_back_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\",\n",
    "                 \"You are an expert at world knowledge. Your task is to step back and paraphrase a question \"\n",
    "                 \"to a more generic step-back question, which is easier to answer. Here are a few examples:\"),\n",
    "                few_shot_prompt,\n",
    "                (\"user\", \"{question}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        question = state[\"messages\"][0].content\n",
    "\n",
    "        llm = self.llm_provider.base()\n",
    "        response = await llm.ainvoke(step_back_prompt.format_messages(question=question))\n",
    "\n",
    "\n",
    "        step_back_question = response.content.strip()\n",
    "        state[\"step_back_question\"] = step_back_question\n",
    "\n",
    "        print(f\"[Step-back Node] Original: {question}\")\n",
    "        print(f\"[Step-back Node] Step-back: {step_back_question}\")\n",
    "        \n",
    "        return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "\n",
    "    async def step_back__ans_generate(self, state: GraphState):\n",
    "        # tool_outputs = [\n",
    "        #         m.content for m in reversed(state[\"messages\"])\n",
    "        #         if isinstance(m, ToolMessage)\n",
    "        #     ][0]\n",
    "        # combined_context = \"\\n\".join(tool_outputs)\n",
    "        combined_context = state.get(\"retrieved_context\",None)\n",
    "        response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "            # {combined_context}\n",
    "            # Original Question: {question}\n",
    "            # Answer:\"\"\"\n",
    "        question = state[\"messages\"][0].content\n",
    "        \n",
    "        response =  await self.llm_provider.base().ainvoke(\n",
    "           response_prompt_template.format(combined_context=combined_context,question=question)\n",
    "        )\n",
    "        return {\"messages\": [response]} \n",
    "\n",
    "\n",
    "\n",
    "    async def generate(self, state: GraphState):\n",
    "        tool_messages = []\n",
    "        print(\"Generating with retrieved context...\")\n",
    "        # print_state(state, \"generate\")\n",
    "        print( \"state in generate: \",state )\n",
    "        for msg in reversed(state[\"messages\"]):\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                tool_messages.append(msg)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        docs_context = \"\\n\".join(m.content for m in reversed(tool_messages))\n",
    "\n",
    "        system = SystemMessage(f\"\"\"\n",
    "            You are a helpful assistant.\n",
    "            Use the retrieved context if relevant.\n",
    "                               \n",
    "            Context:\n",
    "            ---------\n",
    "            {docs_context}\n",
    "            ---------\n",
    "            \"\"\")\n",
    "\n",
    "        convo = [\n",
    "            m for m in state[\"messages\"]\n",
    "            if isinstance(m, (HumanMessage, AIMessage))\n",
    "            and not getattr(m, \"tool_calls\", None)\n",
    "        ]\n",
    "\n",
    "        response =  await self.llm_provider.base().ainvoke(\n",
    "            [system, *convo]\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    async def retrieve(self, state: GraphState):\n",
    "        idx = state[\"current_subq_index\"]\n",
    "        subq = state[\"sub_questions\"][idx]\n",
    "        \n",
    "        print(\n",
    "          f\"idx: {idx} => subq: {subq} ==> qa_pairs: {state['qa_pairs'][:idx]}\"\n",
    "       )\n",
    "\n",
    "\n",
    "        serialized = await self.retrieve_tool.retrieve(query=subq)\n",
    "        \n",
    "        return {\n",
    "           \"retrieved_context\": serialized\n",
    "         }\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    async def generate_decompose(self, state: GraphState):\n",
    "        idx = state[\"current_subq_index\"]\n",
    "        subq = state[\"sub_questions\"][idx]\n",
    "        q_a_pairs  = state.get(\"qa_pairs\", [])[:idx]\n",
    "\n",
    "\n",
    "        question = state[\"messages\"][0].content\n",
    "\n",
    "        # tool_msgs = [m for m in reversed(state[\"messages\"]) if isinstance(m, ToolMessage)][0]\n",
    "        # context = \"\\n\".join(m.content for m in tool_msgs)\n",
    "        context = state.get(\"retrieved_context\", \"\")\n",
    "        \n",
    "\n",
    "        template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "            \\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "            Here is any available background question + answer pairs:\n",
    "\n",
    "            \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "            Here is additional context relevant to the question: \n",
    "\n",
    "            \\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "            Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "            \"\"\"\n",
    "\n",
    "        answer = await self.llm_provider.base().ainvoke(template.format(question=subq,q_a_pairs=q_a_pairs, context= context ))\n",
    "        print(f\"AI Answer: {answer}\")\n",
    "        new_pair = f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "        \n",
    "        return {\n",
    "          \"qa_pairs\": state[\"qa_pairs\"] + [new_pair],\n",
    "          \"current_subq_index\": idx + 1,\n",
    "          \"messages\":[answer]\n",
    "         }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78bdaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "class RAGGraphBuilder:\n",
    "    def __init__(self, nodes: RAGNodes, retrieve_tool: RetrievalTool, checkpointer):\n",
    "        self.nodes = nodes\n",
    "        self.retrieve_tool = retrieve_tool\n",
    "        self.checkpointer = checkpointer\n",
    "    \n",
    "    @staticmethod\n",
    "    def tools_condition(state: GraphState):\n",
    "        last = state[\"messages\"][-1]\n",
    "        print_state(state, \"tools_condition\")\n",
    "        print( \"state in tools_condition: \",state )\n",
    "        if getattr(last, \"tool_calls\", None):\n",
    "            return \"tools\"\n",
    "        return END\n",
    "    @staticmethod\n",
    "    def should_continue(state: GraphState):\n",
    "        if state[\"current_subq_index\"] < len(state[\"sub_questions\"]) or state[\"current_subq_index\"] ==len(state[\"sub_questions\"])-1:\n",
    "            return \"retrieve\"\n",
    "        return END\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        graph = StateGraph(GraphState)\n",
    "\n",
    "        graph.add_node(\"queryOrResponse\", self.nodes.query_or_respond)\n",
    "        # graph.add_node(\"decompose\", self.nodes.decompose)\n",
    "        # graph.add_node(\"tools\", ToolNode(tools=[self.retrieve_tool.tool]))\n",
    "        graph.add_node(\"step_back_retrieve\", self.nodes.step_back_retrieve)\n",
    "        \n",
    "        graph.add_node(\"generate\", self.nodes.generate)\n",
    "        graph.add_node(\"stepBackgenerate\",self.nodes.step_back__ans_generate)\n",
    "        # graph.add_node(\"generateDecompose\",self.nodes.generate_decompose)\n",
    "        # graph.add_node(\"retrieve\",self.nodes.retrieve)\n",
    "        graph.add_node(\"stepBackq\", self.nodes.step_back__q_generate)\n",
    "      \n",
    "        graph.add_edge(START, \"queryOrResponse\")\n",
    "\n",
    "    \n",
    "        graph.add_conditional_edges(\"queryOrResponse\",  self.nodes.route, {\n",
    "            # \"decompose\": \"decompose\",\n",
    "            \"stepBackq\":\"stepBackq\",\n",
    "            \"generate\": \"generate\"\n",
    "        })\n",
    "        graph.add_edge(\"stepBackq\",\"step_back_retrieve\")\n",
    "        graph.add_edge(\"step_back_retrieve\", \"stepBackgenerate\")\n",
    "        graph.add_edge(\"stepBackgenerate\", END)\n",
    "        \n",
    "        \n",
    "        # graph.add_edge(\"tools\", \"generate\")\n",
    "        # graph.add_edge(\"decompose\", \"retrieve\")\n",
    "        # graph.add_edge(\"retrieve\", \"generateDecompose\")\n",
    "        # graph.add_conditional_edges(\n",
    "        #     \"generateDecompose\",\n",
    "        #     self.should_continue,\n",
    "        #     {\n",
    "        #         \"retrieve\": \"retrieve\",\n",
    "        #         END: END\n",
    "        #     }\n",
    "        # )\n",
    "        graph.add_edge(\"generate\", END)\n",
    "\n",
    "        return graph.compile(checkpointer=self.checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578003d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "class RAGPipeline:\n",
    "    def __init__(self, api_key, checkpointer,vectorstore):\n",
    "        self.vectorstore =vectorstore\n",
    "        self.llm_provider = LLMProvider(api_key)\n",
    "        self.checkpointer = checkpointer\n",
    "\n",
    "    async def run(self, query, thread_id, user_id, pdf_id):\n",
    "\n",
    "        retrieve_tool = RetrievalTool(self.vectorstore, self.llm_provider)\n",
    "        nodes = RAGNodes(self.llm_provider, retrieve_tool)\n",
    "\n",
    "        graph = RAGGraphBuilder(\n",
    "            nodes, retrieve_tool, self.checkpointer\n",
    "        ).build()\n",
    "       \n",
    "\n",
    "        print(\"Running RAG graph...\", graph)\n",
    "        config: RunnableConfig = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "        result =  await graph.ainvoke(\n",
    "            {\"messages\": [HumanMessage(query)]},\n",
    "            config\n",
    "        )\n",
    "            \n",
    "        print(result)\n",
    "\n",
    "        return next(\n",
    "            (m.content for m in reversed(result[\"messages\"])\n",
    "             if isinstance(m, AIMessage)),\n",
    "            \"\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e2d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG graph... <langgraph.graph.state.CompiledStateGraph object at 0x71c5cba91d90>\n",
      "\n",
      "=== query_or_respond ===\n",
      "[0] HumanMessage: describe  table 2?\n",
      "==============\n",
      "\n",
      "state in query_or_respond:  {'messages': [HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='d03a1429-444b-4c49-a3ff-6f0cdc4ac73d')]} [SystemMessage(content='Use the retrieve tool if external information is required.', additional_kwargs={}, response_metadata={}), HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='d03a1429-444b-4c49-a3ff-6f0cdc4ac73d')]\n",
      "Response llm:  content='' additional_kwargs={'tool_calls': [{'id': 'tt6w6t3rv', 'function': {'arguments': '{\"query\":\"describe table 2\"}', 'name': 'retrieve'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 316, 'prompt_tokens': 161, 'total_tokens': 477, 'completion_time': 0.676022956, 'completion_tokens_details': {'reasoning_tokens': 291}, 'prompt_time': 0.006896377, 'prompt_tokens_details': None, 'queue_time': 0.159539483, 'total_time': 0.682919333}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b9394-b4d4-7962-bbb4-03ae7de80d7d-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'describe table 2'}, 'id': 'tt6w6t3rv', 'type': 'tool_call'}] usage_metadata={'input_tokens': 161, 'output_tokens': 316, 'total_tokens': 477, 'output_token_details': {'reasoning': 291}}\n",
      "[HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='d03a1429-444b-4c49-a3ff-6f0cdc4ac73d'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'tt6w6t3rv', 'function': {'arguments': '{\"query\":\"describe table 2\"}', 'name': 'retrieve'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 316, 'prompt_tokens': 161, 'total_tokens': 477, 'completion_time': 0.676022956, 'completion_tokens_details': {'reasoning_tokens': 291}, 'prompt_time': 0.006896377, 'prompt_tokens_details': None, 'queue_time': 0.159539483, 'total_time': 0.682919333}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b9394-b4d4-7962-bbb4-03ae7de80d7d-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'describe table 2'}, 'id': 'tt6w6t3rv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 316, 'total_tokens': 477, 'output_token_details': {'reasoning': 291}})] describe  table 2?\n",
      "route decision:  RETRIEVE\n",
      "[Step-back Node] Original: describe  table 2?\n",
      "[Step-back Node] Step-back: what is the purpose of tables in presenting information?\n",
      "[step_back_retrieve] Original: be one of attentiveness and learning.The text \"BIBLIOTECA\" is visible on the wall, suggesting that the room may be \n",
      "part of a library or a section dedicated to books.The presence of educational \n",
      "materials and the organized layout of the room indicate that this is a space \n",
      "designed for learning and reading.<image>\\nParse the figure.Deep Parsing\n",
      "<image>\\n<|grounding|>Convert the document to markdown.Figure 8|For books and articles, the deep parsing mode can output dense captions for natural\n",
      "images in the documents.With just a prompt, the model can automatically identify what type\n",
      "of image it is and output the required results.14 Input image Result\n",
      "<image>\\nParse the figure.Deep Parsing\n",
      "<image>\\n<|grounding|>Convert the document to markdown.Rendering\n",
      "Figure 9 |DeepSeek-OCR in deep parsing mode can also recognize chemical formulas within\n",
      "chemical documents and convert them to SMILES format.In the future, OCR 1.0+2.0 technology\n",
      "may play a significant role in the development of VLM/LLM in STEM fields.15 Input image Result\n",
      "<image>\\nParse the figure.<image>\\n<|grounding|>Convert the document to markdown.RenderingDeep Parsing\n",
      "Figure 10 |DeepSeek-OCR also possesses the capability to copy (structure) simple planar\n",
      "geometric figures.Due to the intricate interdependencies among line segments in geometric\n",
      "shapes, parsing geometry task is extremely challenging and has a long way to go.\n",
      "4.3.2.Multilingual recognition\n",
      "PDF data on the Internet contains not only Chinese and English, but also a large amount of\n",
      "multilingual data, which is also crucial when training LLMs.For PDF documents, DeepSeek-\n",
      "OCR can handle nearly 100 languages.Like Chinese and English documents, multilingual data\n",
      "also supports both layout and non-layout OCR formats.The visualization results are shown in\n",
      "data.For minority languages, in the detection part, we find that the layout model enjoys certain\n",
      "generalization capabilities.In the recognition part, we usefitzto create small patch data to\n",
      "train a GOT-OCR2.0, then use the trained model to label small patches after layout processing,\n",
      "employing a model flywheel to create 600K data samples.During the training of DeepSeek-\n",
      "OCR, coarse labels and fine labels are distinguished using different prompts.The ground truth\n",
      "for fine annotation image-text pairs can be seen in Figure 5.We also collect 3MWorddata,\n",
      "constructing high-quality image-text pairs without layout by directly extracting content.This\n",
      "data mainly brings benefits to formulas and HTML-formatted tables.Additionally, we select\n",
      "some open-source data[28, 37]as supplements.For natural scene OCR, our model mainly supports Chinese and English.The image data\n",
      "sources come from LAION [31] and Wukong[13], labeled using PaddleOCR[9], with 10M data\n",
      "samples each for Chinese and English.Like document OCR, natural scene OCR can also control\n",
      "whether to output detection boxes through prompts.\n",
      "3.4.2.OCR 2.0 data\n",
      "Following GOT-OCR2.0[38], we refer to chart, chemical formula, and plane geometry parsing\n",
      "data as OCR 2.0 data.For chart data, following OneChart[7], we use pyecharts and matplotlib\n",
      "8 (a) Image-text ground truth of chart\n",
      " (b) Image-text ground truth of geometry\n",
      "Figure 6 |For charts, we do not use OneChart’s[7] dictionary format, but instead use HTML\n",
      "table format as labels, which can save a certain amount of tokens.For plane geometry, we\n",
      "convert the ground truth to dictionary format, where the dictionary contains keys such as\n",
      "line segments, endpoint coordinates, line segment types, etc., for better readability.Each line\n",
      "segment is encoded using the Slow Perception [39] manner.to render 10M images, mainly including commonly used line, bar, pie, and composite charts.We define chart parsing as image-to-HTML-table conversion task, as shown in Figure 6(a).For\n",
      "[step_back_retrieve] Step-back: be one of attentiveness and learning.The text \"BIBLIOTECA\" is visible on the wall, suggesting that the room may be \n",
      "part of a library or a section dedicated to books.The presence of educational \n",
      "materials and the organized layout of the room indicate that this is a space \n",
      "designed for learning and reading.<image>\\nParse the figure.Deep Parsing\n",
      "<image>\\n<|grounding|>Convert the document to markdown.Figure 8|For books and articles, the deep parsing mode can output dense captions for natural\n",
      "images in the documents.With just a prompt, the model can automatically identify what type\n",
      "of image it is and output the required results.14 Input image Result\n",
      "<image>\\nParse the figure.Deep Parsing\n",
      "<image>\\n<|grounding|>Convert the document to markdown.Rendering\n",
      "Figure 9 |DeepSeek-OCR in deep parsing mode can also recognize chemical formulas within\n",
      "chemical documents and convert them to SMILES format.In the future, OCR 1.0+2.0 technology\n",
      "may play a significant role in the development of VLM/LLM in STEM fields.15 Input image Result\n",
      "<image>\\nParse the figure.<image>\\n<|grounding|>Convert the document to markdown.RenderingDeep Parsing\n",
      "Figure 10 |DeepSeek-OCR also possesses the capability to copy (structure) simple planar\n",
      "geometric figures.Due to the intricate interdependencies among line segments in geometric\n",
      "shapes, parsing geometry task is extremely challenging and has a long way to go.\n",
      "4.3.2.Multilingual recognition\n",
      "PDF data on the Internet contains not only Chinese and English, but also a large amount of\n",
      "multilingual data, which is also crucial when training LLMs.For PDF documents, DeepSeek-\n",
      "OCR can handle nearly 100 languages.Like Chinese and English documents, multilingual data\n",
      "also supports both layout and non-layout OCR formats.The visualization results are shown in\n",
      "data.For minority languages, in the detection part, we find that the layout model enjoys certain\n",
      "generalization capabilities.In the recognition part, we usefitzto create small patch data to\n",
      "train a GOT-OCR2.0, then use the trained model to label small patches after layout processing,\n",
      "employing a model flywheel to create 600K data samples.During the training of DeepSeek-\n",
      "OCR, coarse labels and fine labels are distinguished using different prompts.The ground truth\n",
      "for fine annotation image-text pairs can be seen in Figure 5.We also collect 3MWorddata,\n",
      "constructing high-quality image-text pairs without layout by directly extracting content.This\n",
      "data mainly brings benefits to formulas and HTML-formatted tables.Additionally, we select\n",
      "some open-source data[28, 37]as supplements.For natural scene OCR, our model mainly supports Chinese and English.The image data\n",
      "sources come from LAION [31] and Wukong[13], labeled using PaddleOCR[9], with 10M data\n",
      "samples each for Chinese and English.Like document OCR, natural scene OCR can also control\n",
      "whether to output detection boxes through prompts.\n",
      "3.4.2.OCR 2.0 data\n",
      "Following GOT-OCR2.0[38], we refer to chart, chemical formula, and plane geometry parsing\n",
      "data as OCR 2.0 data.For chart data, following OneChart[7], we use pyecharts and matplotlib\n",
      "8 (a) Image-text ground truth of chart\n",
      " (b) Image-text ground truth of geometry\n",
      "Figure 6 |For charts, we do not use OneChart’s[7] dictionary format, but instead use HTML\n",
      "table format as labels, which can save a certain amount of tokens.For plane geometry, we\n",
      "convert the ground truth to dictionary format, where the dictionary contains keys such as\n",
      "line segments, endpoint coordinates, line segment types, etc., for better readability.Each line\n",
      "segment is encoded using the Slow Perception [39] manner.to render 10M images, mainly including commonly used line, bar, pie, and composite charts.We define chart parsing as image-to-HTML-table conversion task, as shown in Figure 6(a).For\n",
      "{'messages': [HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='d03a1429-444b-4c49-a3ff-6f0cdc4ac73d'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'tt6w6t3rv', 'function': {'arguments': '{\"query\":\"describe table 2\"}', 'name': 'retrieve'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 316, 'prompt_tokens': 161, 'total_tokens': 477, 'completion_time': 0.676022956, 'completion_tokens_details': {'reasoning_tokens': 291}, 'prompt_time': 0.006896377, 'prompt_tokens_details': None, 'queue_time': 0.159539483, 'total_time': 0.682919333}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b9394-b4d4-7962-bbb4-03ae7de80d7d-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'describe table 2'}, 'id': 'tt6w6t3rv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 316, 'total_tokens': 477, 'output_token_details': {'reasoning': 291}}), AIMessage(content='The provided context does not explicitly describe or reference **Table 2**. The text focuses on topics such as OCR technologies (e.g., DeepSeek-OCR, GOT-OCR2.0), deep parsing capabilities (e.g., handling chemical formulas, geometric figures, charts), multilingual recognition, and data sources (e.g., LAION, Wukong). While figures like **Figure 6**, **Figure 8**, **Figure 9**, and **Figure 10** are discussed in detail, there is no mention of **Table 2** in the given content. If Table 2 exists in an associated document or image not included here, additional context would be required to describe it.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 325, 'prompt_tokens': 1794, 'total_tokens': 2119, 'completion_time': 0.653012171, 'completion_tokens_details': {'reasoning_tokens': 178}, 'prompt_time': 0.090593897, 'prompt_tokens_details': None, 'queue_time': 0.056808502, 'total_time': 0.743606068}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b9394-c37c-7853-a6eb-0a15d74be9cd-0', usage_metadata={'input_tokens': 1794, 'output_tokens': 325, 'total_tokens': 2119, 'output_token_details': {'reasoning': 178}})], 'retrieved_context': 'be one of attentiveness and learning.The text \"BIBLIOTECA\" is visible on the wall, suggesting that the room may be \\npart of a library or a section dedicated to books.The presence of educational \\nmaterials and the organized layout of the room indicate that this is a space \\ndesigned for learning and reading.<image>\\\\nParse the figure.Deep Parsing\\n<image>\\\\n<|grounding|>Convert the document to markdown.Figure 8|For books and articles, the deep parsing mode can output dense captions for natural\\nimages in the documents.With just a prompt, the model can automatically identify what type\\nof image it is and output the required results.14 Input image Result\\n<image>\\\\nParse the figure.Deep Parsing\\n<image>\\\\n<|grounding|>Convert the document to markdown.Rendering\\nFigure 9 |DeepSeek-OCR in deep parsing mode can also recognize chemical formulas within\\nchemical documents and convert them to SMILES format.In the future, OCR 1.0+2.0 technology\\nmay play a significant role in the development of VLM/LLM in STEM fields.15 Input image Result\\n<image>\\\\nParse the figure.<image>\\\\n<|grounding|>Convert the document to markdown.RenderingDeep Parsing\\nFigure 10 |DeepSeek-OCR also possesses the capability to copy (structure) simple planar\\ngeometric figures.Due to the intricate interdependencies among line segments in geometric\\nshapes, parsing geometry task is extremely challenging and has a long way to go.\\n4.3.2.Multilingual recognition\\nPDF data on the Internet contains not only Chinese and English, but also a large amount of\\nmultilingual data, which is also crucial when training LLMs.For PDF documents, DeepSeek-\\nOCR can handle nearly 100 languages.Like Chinese and English documents, multilingual data\\nalso supports both layout and non-layout OCR formats.The visualization results are shown in\\ndata.For minority languages, in the detection part, we find that the layout model enjoys certain\\ngeneralization capabilities.In the recognition part, we usefitzto create small patch data to\\ntrain a GOT-OCR2.0, then use the trained model to label small patches after layout processing,\\nemploying a model flywheel to create 600K data samples.During the training of DeepSeek-\\nOCR, coarse labels and fine labels are distinguished using different prompts.The ground truth\\nfor fine annotation image-text pairs can be seen in Figure 5.We also collect 3MWorddata,\\nconstructing high-quality image-text pairs without layout by directly extracting content.This\\ndata mainly brings benefits to formulas and HTML-formatted tables.Additionally, we select\\nsome open-source data[28, 37]as supplements.For natural scene OCR, our model mainly supports Chinese and English.The image data\\nsources come from LAION [31] and Wukong[13], labeled using PaddleOCR[9], with 10M data\\nsamples each for Chinese and English.Like document OCR, natural scene OCR can also control\\nwhether to output detection boxes through prompts.\\n3.4.2.OCR 2.0 data\\nFollowing GOT-OCR2.0[38], we refer to chart, chemical formula, and plane geometry parsing\\ndata as OCR 2.0 data.For chart data, following OneChart[7], we use pyecharts and matplotlib\\n8 (a) Image-text ground truth of chart\\n (b) Image-text ground truth of geometry\\nFigure 6 |For charts, we do not use OneChart’s[7] dictionary format, but instead use HTML\\ntable format as labels, which can save a certain amount of tokens.For plane geometry, we\\nconvert the ground truth to dictionary format, where the dictionary contains keys such as\\nline segments, endpoint coordinates, line segment types, etc., for better readability.Each line\\nsegment is encoded using the Slow Perception [39] manner.to render 10M images, mainly including commonly used line, bar, pie, and composite charts.We define chart parsing as image-to-HTML-table conversion task, as shown in Figure 6(a).For\\nbe one of attentiveness and learning.The text \"BIBLIOTECA\" is visible on the wall, suggesting that the room may be \\npart of a library or a section dedicated to books.The presence of educational \\nmaterials and the organized layout of the room indicate that this is a space \\ndesigned for learning and reading.<image>\\\\nParse the figure.Deep Parsing\\n<image>\\\\n<|grounding|>Convert the document to markdown.Figure 8|For books and articles, the deep parsing mode can output dense captions for natural\\nimages in the documents.With just a prompt, the model can automatically identify what type\\nof image it is and output the required results.14 Input image Result\\n<image>\\\\nParse the figure.Deep Parsing\\n<image>\\\\n<|grounding|>Convert the document to markdown.Rendering\\nFigure 9 |DeepSeek-OCR in deep parsing mode can also recognize chemical formulas within\\nchemical documents and convert them to SMILES format.In the future, OCR 1.0+2.0 technology\\nmay play a significant role in the development of VLM/LLM in STEM fields.15 Input image Result\\n<image>\\\\nParse the figure.<image>\\\\n<|grounding|>Convert the document to markdown.RenderingDeep Parsing\\nFigure 10 |DeepSeek-OCR also possesses the capability to copy (structure) simple planar\\ngeometric figures.Due to the intricate interdependencies among line segments in geometric\\nshapes, parsing geometry task is extremely challenging and has a long way to go.\\n4.3.2.Multilingual recognition\\nPDF data on the Internet contains not only Chinese and English, but also a large amount of\\nmultilingual data, which is also crucial when training LLMs.For PDF documents, DeepSeek-\\nOCR can handle nearly 100 languages.Like Chinese and English documents, multilingual data\\nalso supports both layout and non-layout OCR formats.The visualization results are shown in\\ndata.For minority languages, in the detection part, we find that the layout model enjoys certain\\ngeneralization capabilities.In the recognition part, we usefitzto create small patch data to\\ntrain a GOT-OCR2.0, then use the trained model to label small patches after layout processing,\\nemploying a model flywheel to create 600K data samples.During the training of DeepSeek-\\nOCR, coarse labels and fine labels are distinguished using different prompts.The ground truth\\nfor fine annotation image-text pairs can be seen in Figure 5.We also collect 3MWorddata,\\nconstructing high-quality image-text pairs without layout by directly extracting content.This\\ndata mainly brings benefits to formulas and HTML-formatted tables.Additionally, we select\\nsome open-source data[28, 37]as supplements.For natural scene OCR, our model mainly supports Chinese and English.The image data\\nsources come from LAION [31] and Wukong[13], labeled using PaddleOCR[9], with 10M data\\nsamples each for Chinese and English.Like document OCR, natural scene OCR can also control\\nwhether to output detection boxes through prompts.\\n3.4.2.OCR 2.0 data\\nFollowing GOT-OCR2.0[38], we refer to chart, chemical formula, and plane geometry parsing\\ndata as OCR 2.0 data.For chart data, following OneChart[7], we use pyecharts and matplotlib\\n8 (a) Image-text ground truth of chart\\n (b) Image-text ground truth of geometry\\nFigure 6 |For charts, we do not use OneChart’s[7] dictionary format, but instead use HTML\\ntable format as labels, which can save a certain amount of tokens.For plane geometry, we\\nconvert the ground truth to dictionary format, where the dictionary contains keys such as\\nline segments, endpoint coordinates, line segment types, etc., for better readability.Each line\\nsegment is encoded using the Slow Perception [39] manner.to render 10M images, mainly including commonly used line, bar, pie, and composite charts.We define chart parsing as image-to-HTML-table conversion task, as shown in Figure 6(a).For'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The provided context does not explicitly describe or reference **Table 2**. The text focuses on topics such as OCR technologies (e.g., DeepSeek-OCR, GOT-OCR2.0), deep parsing capabilities (e.g., handling chemical formulas, geometric figures, charts), multilingual recognition, and data sources (e.g., LAION, Wukong). While figures like **Figure 6**, **Figure 8**, **Figure 9**, and **Figure 10** are discussed in detail, there is no mention of **Table 2** in the given content. If Table 2 exists in an associated document or image not included here, additional context would be required to describe it.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = RAGPipeline(\n",
    "    api_key=API_KEY_GROQ,\n",
    "    checkpointer=checkpointer,\n",
    "    vectorstore=memory_store\n",
    ")\n",
    "result =  await pipeline.run(\"describe  table 2?\", thread_id=\"thread1\", user_id=\"user1\", pdf_id=\"pdf1\" )\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d653bb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answer:** The provided context does not explicitly describe or reference **Table 2**. The text focuses on topics such as OCR technologies (e.g., DeepSeek-OCR, GOT-OCR2.0), deep parsing capabilities (e.g., handling chemical formulas, geometric figures, charts), multilingual recognition, and data sources (e.g., LAION, Wukong). While figures like **Figure 6**, **Figure 8**, **Figure 9**, and **Figure 10** are discussed in detail, there is no mention of **Table 2** in the given content. If Table 2 exists in an associated document or image not included here, additional context would be required to describe it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"**Answer:** {result}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9acb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGoogleGenerativeAI(\n",
    "#             model=\"gemini-2.5-flash\",\n",
    "#             temperature=0,\n",
    "#             api_key=api_key1,\n",
    "#         )\n",
    "\n",
    "# llm.invoke(ROUTER_PROMPT.format(question=\"What is the contribution of the paper ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14957d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea120b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = memory_store.similarity_search(\"ocr\", k=3)\n",
    "\n",
    "# print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dadc2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EuroEval/gemma-3-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b8949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def get_chunks_with_ids(text):\n",
    "#     cleaned_text = re.sub(r\"(Contents).*?1\\. Introduction\", r\"\\1\\n\", text, flags=re.DOTALL)\n",
    "#     cleaned_text = re.sub(r\"(References|REFERENCES).*?$\", \"\", cleaned_text, flags=re.DOTALL)\n",
    "#     sentences = split_sentences(cleaned_text)\n",
    "#     text_sentences = [s for s in sentences if maybe_is_text(s)]\n",
    "\n",
    "#     currentIndex = 0\n",
    "#     chunks_with_ids = []\n",
    "#     for i, s in enumerate(text_sentences):\n",
    "#         start = currentIndex\n",
    "#         end = start + len(s)\n",
    "\n",
    "#         chunks_with_ids.append((i, start, end, s))\n",
    "#         currentIndex = end+1  # +1 for the space/newline between sentences\n",
    "    \n",
    "\n",
    "#     return chunks_with_ids\n",
    "        \n",
    "\n",
    "\n",
    "# def createChunksPreservingId (chunks_with_ids, tokenizer, maxTokens, buffer):\n",
    "#     chunks = []\n",
    "#     currentTokens =0\n",
    "#     currentChunk = []\n",
    "\n",
    "#     for chunk in chunks_with_ids:\n",
    "#         tokenCount = len(tokenizer.encode(chunk[3], add_special_tokens=False))\n",
    "#         if currentTokens + tokenCount +buffer > maxTokens:\n",
    "#             if  len(currentChunk) > 0:\n",
    "#                chunks.append([currentChunk])\n",
    "#             currentChunk = [chunk]\n",
    "#             currentTokens = tokenCount\n",
    "\n",
    "#         else:\n",
    "#             currentChunk.append(chunk)\n",
    "#             currentTokens += tokenCount\n",
    "#     if  len(currentChunk) > 0:\n",
    "#                chunks.append([currentChunk])\n",
    "#     return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b5b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_ids = get_chunks_with_ids(cleaned_text)\n",
    "# chunks = createChunksPreservingId(sentences_ids, tokenizer, maxTokens=500, buffer=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
