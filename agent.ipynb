{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad29ed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# #https://github.com/Future-House/paper-qa.git\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# # Only run this block for Vertex AI API\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# #https://github.com/Future-House/paper-qa.git\n",
    "from google.genai import types\n",
    "from google import genai\n",
    "\n",
    "# # Only run this block for Vertex AI API\n",
    "client = genai.Client(api_key=\"AIzaSyDn82tmjyQRkZr3K79CXWC47ab3Xko29L0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01863a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU langchain-community arxiv pymupdf langchain pypdf\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install -U transformers\n",
    "# !pip install -qU langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4e57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "def get_text_from_pdf(file_path=None, url=None):\n",
    "    if url:\n",
    "        response = requests.get(url)\n",
    "        pdf_bytes = response.content\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_pdf:\n",
    "            temp_pdf.write(pdf_bytes)\n",
    "            file_path = temp_pdf.name\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split()\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_text_from_pdf(url=\"https://arxiv.org/pdf/2510.18234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd73534",
   "metadata": {},
   "source": [
    "## Chunking techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9441fa",
   "metadata": {},
   "source": [
    "### 1. Sliding Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabffaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [ doc.page_content for doc in docs ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a34528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/gemini-embedding-001\",\n",
    "            google_api_key=\"AIzaSyDn82tmjyQRkZr3K79CXWC47ab3Xko29L0\"\n",
    "        )\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=150,\n",
    "        )\n",
    "\n",
    "text = \"\".join(pages)\n",
    "docs = splitter.create_documents([text])\n",
    "print(len(docs))\n",
    "class MemoryStore():\n",
    "    def __init__(self, embeddings):\n",
    "        self.store = InMemoryVectorStore(embedding=embeddings)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "      \n",
    "        return  self.store.add_documents(documents)\n",
    "\n",
    "    def similarity_search(self, query, k=4):\n",
    "        return self.store.similarity_search(query, k)\n",
    "    \n",
    "\n",
    "memory_store = MemoryStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47da6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 29 documents to memory store\n"
     ]
    }
   ],
   "source": [
    "ids = memory_store.add_documents(docs)\n",
    "print(f\"Added {len(ids)} documents to memory store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea120b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "results = memory_store.similarity_search(\"ocr\", k=3)\n",
    "\n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0b6707c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d65bc5c1-9984-4730-99f8-2160844657f2', metadata={}, page_content='PPstructure-v3 [9] - 0.152 0.073 0.295 0.162 0.077 0.223 0.136 0.535 0.111 0.11\\nEnd-to-end Models\\nNougat [6] 2352 0.452 0.365 0.488 0.572 0.382 0.973 0.998 0.941 1.00 0.954\\nSmolDocling [25] 392 0.493 0.262 0.753 0.729 0.227 0.816 0.838 0.997 0.907 0.522\\nInternVL2-76B [8] 6790 0.44 0.353 0.543 0.547 0.317 0.443 0.29 0.701 0.555 0.228\\nQwen2.5-VL-7B [5] 3949 0.316 0.151 0.376 0.598 0.138 0.399 0.243 0.5 0.627 0.226\\nOLMOCR [28] 3949 0.326 0.097 0.455 0.608 0.145 0.469 0.293 0.655 0.652 0.277\\nGOT-OCR2.0 [38] 256 0.287 0.189 0.360 0.459 0.141 0.411 0.315 0.528 0.52 0.28\\nOCRFlux-3B [3] 3949 0.238 0.112 0.447 0.269 0.126 0.349 0.256 0.716 0.162 0.263\\nGPT4o [26] - 0.233 0.144 0.425 0.234 0.128 0.399 0.409 0.606 0.329 0.251\\nInternVL3-78B [42] 6790 0.218 0.117 0.38 0.279 0.095 0.296 0.21 0.533 0.282 0.161\\nQwen2.5-VL-72B [5] 3949 0.214 0.092 0.315 0.341 0.106 0.261 0.18 0.434 0.262 0.168\\ndots.ocr [30] 3949 0.182 0.137 0.320 0.166 0.182 0.261 0.229 0.468 0.160 0.261\\nGemini2.5-Pro [4] - 0.148 0.055 0.356 0.13 0.049 0.212 0.168 0.439 0.119 0.121\\nMinerU2.0 [34] 6790 0.133 0.045 0.273 0.15 0.066 0.238 0.115 0.506 0.209 0.122\\ndots.ocr†200dpi [30] 5545 0.1250.0320.3290.099 0.040.160.0660.416 0.0920.067\\nDeepSeek-OCR (end2end)\\nTiny640.386 0.373 0.469 0.422 0.283 0.361 0.307 0.635 0.266 0.236\\nSmall 100 0.221 0.142 0.373 0.242 0.125 0.284 0.24 0.53 0.159 0.205\\nBase 256(182) 0.137 0.054 0.267 0.163 0.064 0.24 0.205 0.474 0.1 0.181\\nLarge 400(285) 0.138 0.054 0.277 0.152 0.067 0.208 0.143 0.461 0.104 0.123\\nGundam 795 0.127 0.043 0.269 0.134 0.062 0.181 0.097 0.432 0.089 0.103\\nGundam-M†200dpi 18530.1230.0490.2420.147 0.0560.1570.0870.377 0.080.085\\nwithout layout: \"<image>\\\\nFree OCR.\" to control the model’s output format. Nevertheless, the\\noutput format still cannot completely match Fox benchmarks, so the actual performance would\\nbe somewhat higher than the test results.\\nAs shown in Table 2, within a 10×compression ratio, the model’s decoding precision can'),\n",
       " Document(id='1c8482f1-a3a8-4ca1-83a3-d39a8e279182', metadata={}, page_content='4.2. OCR Practical Performance\\nDeepSeek-OCR is not only an experimental model; it has strong practical capabilities and can\\nconstruct data for LLM/VLM pretraining. To quantify OCR performance, we test DeepSeek-\\nOCR on OmniDocBench [27], with results shown in Table 3. Requiring only 100 vision tokens\\n(640×640 resolution), DeepSeek-OCR surpasses GOT-OCR2.0 [38] which uses 256 tokens; with\\n400 tokens (285 valid tokens, 1280×1280 resolution), it achieves on-par performance with state-\\nof-the-arts on this benchmark. Using fewer than 800 tokens (Gundam mode), DeepSeek-OCR\\noutperforms MinerU2.0 [34] which needs nearly 7,000 vision tokens. These results demonstrate\\nthat our DeepSeek-OCR model is powerful in practical applications, and because the higher\\ntokens compression, it enjoys a higher research ceiling.\\nAs shown in Table 4, some categories of documents require very few tokens to achieve\\nsatisfactory performance, such as slides which only need 64 vision tokens. For book and\\nreport documents, DeepSeek-OCR can achieve good performance with only 100 vision tokens.\\nCombined with the analysis from Section 4.1, this may be because most text tokens in these\\ndocument categories are within 1,000, meaning the vision-token compression ratio does not\\nexceed 10×. For newspapers, Gundam or even Gundam-master mode is required to achieve\\nacceptable edit distances, because the text tokens in newspapers are 4-5,000, far exceeding the\\n10×compression of other modes. These experimental results further demonstrate the boundaries\\nof contexts optical compression, which may provide effective references for researches on the\\nvision token optimization in VLMs and context compression, forgetting mechanisms in LLMs.\\n4.3. Qualitative Study\\n4.3.1. Deep parsing\\nDeepSeek-OCR possesses both layout and OCR 2.0 capabilities, enabling it to further parse\\nimages within documents through secondary model calls, a feature we refer to as \"deep parsing\".'),\n",
       " Document(id='c884bdcb-d5a9-4618-a95e-d7fe3fe1dcb0', metadata={}, page_content='production, serving as an indispensable assistant for LLMs. Of course, OCR alone is insufficient\\nto fully validate true context optical compression and we will conduct digital-optical text in-\\nterleaved pretraining, needle-in-a-haystack testing, and other evaluations in the future. From\\nanother perspective, optical contexts compression still offers substantial room for research and\\nimprovement, representing a promising new direction.\\n19References\\n[1] Marker. URLhttps://github.com/datalab-to/marker.\\n[2] Mathpix. URLhttps://mathpix.com/.\\n[3] Ocrflux, 2025. URLhttps://github.com/chatdoc-com/OCRFlux.\\n[4] G. AI. Gemini 2.5-pro, 2025. URLhttps://gemini.google.com/.\\n[5] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P . Wang, S. Wang, J. Tang,\\nH. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P . Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang,\\nT. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv\\npreprint arXiv:2502.13923, 2025.\\n[6] L. Blecher, G. Cucurull, T. Scialom, and R. Stojnic. Nougat: Neural optical understanding\\nfor academic documents. arXiv preprint arXiv:2308.13418, 2023.\\n[7] J. Chen, L. Kong, H. Wei, C. Liu, Z. Ge, L. Zhao, J. Sun, C. Han, and X. Zhang. Onechart:\\nPurify the chart structural extraction via one auxiliary token. In Proceedings of the 32nd\\nACM International Conference on Multimedia, pages 147–155, 2024.\\n[8] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How\\nfar are we to gpt-4v? closing the gap to commercial multimodal models with open-source\\nsuites. arXiv preprint arXiv:2404.16821, 2024.\\n[9] C. Cui, T. Sun, M. Lin, T. Gao, Y. Zhang, J. Liu, X. Wang, Z. Zhang, C. Zhou, H. Liu, et al.\\nPaddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025.\\n[10] M. Dehghani, J. Djolonga, B. Mustafa, P . Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron,\\nR. Geirhos, I. Alabdulmohsin, et al. Patch n’ pack: Navit, a vision transformer for any aspect')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71179195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171ee9b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cleaned_text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(Contents).*?1\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m. Introduction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtext\u001b[49m, flags=re.DOTALL)\n\u001b[32m      4\u001b[39m cleaned_text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(References|REFERENCES).*?$\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, cleaned_text, flags=re.DOTALL)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# display(Markdown(cleaned_text))\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import re\n",
    "cleaned_text = re.sub(r\"(Contents).*?1\\. Introduction\", r\"\\1\\n\", text, flags=re.DOTALL)\n",
    "cleaned_text = re.sub(r\"(References|REFERENCES).*?$\", \"\", cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "\n",
    "# display(Markdown(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "950fa0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "MAX_TEXT_ENTROPY = 8.0\n",
    "\n",
    "def maybe_is_text(s: str, thresh: float = 2.5) -> bool:\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the string to discard files with excessively repeated symbols.\n",
    "\n",
    "    PDF parsing sometimes represents horizontal distances between words on title pages\n",
    "    and in tables with spaces, which should therefore not be included in this calculation.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    s_wo_spaces = s.replace(\" \", \"\")\n",
    "    if not s_wo_spaces:\n",
    "        return False\n",
    "\n",
    "    counts = Counter(s_wo_spaces)\n",
    "    entropy = 0.0\n",
    "    length = len(s_wo_spaces)\n",
    "    for count in counts.values():\n",
    "        p = count / length\n",
    "        entropy += -p * math.log2(p)\n",
    "\n",
    "    # Check if the entropy is within a reasonable range for text\n",
    "    return MAX_TEXT_ENTROPY > entropy > thresh\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dadc2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EuroEval/gemma-3-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0b8949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_chunks_with_ids(text):\n",
    "    cleaned_text = re.sub(r\"(Contents).*?1\\. Introduction\", r\"\\1\\n\", text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r\"(References|REFERENCES).*?$\", \"\", cleaned_text, flags=re.DOTALL)\n",
    "    sentences = split_sentences(cleaned_text)\n",
    "    text_sentences = [s for s in sentences if maybe_is_text(s)]\n",
    "\n",
    "    currentIndex = 0\n",
    "    chunks_with_ids = []\n",
    "    for i, s in enumerate(text_sentences):\n",
    "        start = currentIndex\n",
    "        end = start + len(s)\n",
    "\n",
    "        chunks_with_ids.append((i, start, end, s))\n",
    "        currentIndex = end+1  # +1 for the space/newline between sentences\n",
    "    \n",
    "\n",
    "    return chunks_with_ids\n",
    "        \n",
    "\n",
    "\n",
    "def createChunksPreservingId (chunks_with_ids, tokenizer, maxTokens, buffer):\n",
    "    chunks = []\n",
    "    currentTokens =0\n",
    "    currentChunk = []\n",
    "\n",
    "    for chunk in chunks_with_ids:\n",
    "        tokenCount = len(tokenizer.encode(chunk[3], add_special_tokens=False))\n",
    "        if currentTokens + tokenCount +buffer > maxTokens:\n",
    "            if  len(currentChunk) > 0:\n",
    "               chunks.append([currentChunk])\n",
    "            currentChunk = [chunk]\n",
    "            currentTokens = tokenCount\n",
    "\n",
    "        else:\n",
    "            currentChunk.append(chunk)\n",
    "            currentTokens += tokenCount\n",
    "    if  len(currentChunk) > 0:\n",
    "               chunks.append([currentChunk])\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5b5b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_ids = get_chunks_with_ids(cleaned_text)\n",
    "chunks = createChunksPreservingId(sentences_ids, tokenizer, maxTokens=500, buffer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d4d9fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 3\u001b[0m env_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.env\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "env_file = Path(__file__).resolve().parents[2] / \".env\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
