{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad29ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #https://github.com/Future-House/paper-qa.git\n",
    "# from google.genai import types\n",
    "# from google import genai\n",
    "\n",
    "# # # Only run this block for Vertex AI API\n",
    "# client = genai.Client(api_key=\"AIzaSyDn82tmjyQRkZr3K79CXWC47ab3Xko29L0\")\n",
    "\n",
    "# !pip install py-spy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Get a single environment variable\n",
    "API_KEY_GROQ = os.getenv(\"API_KEY_GROQ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01863a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-community arxiv pymupdf langchain pypdf\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install -U transformers\n",
    "# !pip install -qU langchain_google_genai\n",
    "# !pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4e57d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanje/.pyenv/versions/3.12.3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "def get_text_from_pdf(file_path=None, url=None):\n",
    "    if url:\n",
    "        response = requests.get(url)\n",
    "        pdf_bytes = response.content\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_pdf:\n",
    "            temp_pdf.write(pdf_bytes)\n",
    "            file_path = temp_pdf.name\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split()\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_text_from_pdf(url=\"https://arxiv.org/pdf/2510.18234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd73534",
   "metadata": {},
   "source": [
    "## Chunking techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9441fa",
   "metadata": {},
   "source": [
    "### 1. Sliding Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabffaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [ doc.page_content for doc in docs ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998d6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text:str) :\n",
    "\n",
    "    cleaned_text = re.sub(r\"(Contents).*?1\\. Introduction\", r\"\\1\\n\", text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r\"(References|REFERENCES).*?$\", \"\", cleaned_text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96110956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text = clean_text(' '.join(pages))\n",
    "# display(Markdown(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d13784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "MAX_TEXT_ENTROPY = 8.0\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def maybe_is_text(s: str, thresh: float = 2.5) -> bool:\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the string to discard files with excessively repeated symbols.\n",
    "\n",
    "    PDF parsing sometimes represents horizontal distances between words on title pages\n",
    "    and in tables with spaces, which should therefore not be included in this calculation.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    s_wo_spaces = s.replace(\" \", \"\")\n",
    "    if not s_wo_spaces:\n",
    "        return False\n",
    "\n",
    "    counts = Counter(s_wo_spaces)\n",
    "    entropy = 0.0\n",
    "    length = len(s_wo_spaces)\n",
    "    for count in counts.values():\n",
    "        p = count / length\n",
    "        entropy += -p * math.log2(p)\n",
    "\n",
    "    # Check if the entropy is within a reasonable range for text\n",
    "    return MAX_TEXT_ENTROPY > entropy > thresh\n",
    "\n",
    "\n",
    "sentences = split_sentences(clean_text(' '.join(pages)))\n",
    "text_sentences = [s for s in sentences if maybe_is_text(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a34528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/gemini-embedding-001\",\n",
    "             google_api_key=\"AIzaSyDEckSvtc3k_d0KgXyPgsvC1nUUjYc7xBk\",\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=150,\n",
    "        )\n",
    "\n",
    "text = ''.join(text_sentences)\n",
    "docs = splitter.create_documents([text])\n",
    "print(len(docs))\n",
    "class MemoryStore():\n",
    "    def __init__(self, embeddings):\n",
    "        self.store = InMemoryVectorStore(embedding=embeddings)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "      \n",
    "        return  self.store.add_documents(documents)\n",
    "\n",
    "    def similarity_search(self, query, k=4):\n",
    "        return self.store.similarity_search(query, k)\n",
    "    \n",
    "\n",
    "memory_store = MemoryStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47da6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 24 documents to memory store\n"
     ]
    }
   ],
   "source": [
    "ids = memory_store.add_documents(docs)\n",
    "print(f\"Added {len(ids)} documents to memory store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fba2baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state(state, node_name):\n",
    "    print(f\"\\n=== {node_name} ===\")\n",
    "    for i, m in enumerate(state[\"messages\"]):\n",
    "        print(f\"[{i}] {m.__class__.__name__}: {m.content}\")\n",
    "        if getattr(m, \"tool_calls\", None):\n",
    "            print(\"   tool_calls:\", m.tool_calls)\n",
    "    print(\"==============\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd69211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class LLMProvider:\n",
    "    def __init__(self, api_key: str):\n",
    "        # self.llm = ChatGoogleGenerativeAI(\n",
    "        #     model=\"gemini-2.5-flash\",\n",
    "        #     temperature=0,\n",
    "        #     api_key=api_key,\n",
    "        # )\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"qwen/qwen3-32b\",\n",
    "            temperature=0,\n",
    "            reasoning_format=\"hidden\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "    def base(self):\n",
    "        return self.llm\n",
    "    \n",
    "    def with_tools(self, tools):\n",
    "        return self.llm.bind_tools(tools, tool_choice=\"retrieve\") \n",
    "    \n",
    "\n",
    "class RetrievalTool:\n",
    "    def __init__(self, vectorstore, llm_provider:LLMProvider):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm_provider = llm_provider\n",
    "        self.tool = self._build_tool()\n",
    "        self.MULTI_QUERY_PROMPT =  \"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "                    different versions of the given user question to retrieve relevant documents from a vector \n",
    "                    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "                    the user overcome some of the limitations of the distance-based similarity search. \n",
    "                    Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "\n",
    "    def get_unique_union(self, results):\n",
    "        \"\"\" Unique union of retrieved docs \"\"\"\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for docs in results:\n",
    "            for doc in docs: \n",
    "                if isinstance(doc, tuple):\n",
    "                   doc = doc[0]\n",
    "                if doc.page_content not in seen:\n",
    "                    seen.add(doc.page_content)\n",
    "                unique_docs.append(doc.page_content)\n",
    "        return unique_docs\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def reciprocal_rank_fusion(self, results: list[list], k=60):\n",
    "        \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "            and an optional parameter k used in the RRF formula \"\"\"\n",
    "        # Initialize a dictionary to hold fused scores for each unique document\n",
    "        fused_scores = {}\n",
    "        for docs in results:\n",
    "            print(f\"len of docs : {len(docs)}\")\n",
    "            for rank, doc in enumerate(docs):\n",
    "                doc_str= doc.page_content\n",
    "                # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "                if doc_str not in fused_scores:\n",
    "                    fused_scores[doc_str] = 0\n",
    "                # Retrieve the current score of the document, if any\n",
    "                previous_score = fused_scores[doc_str]\n",
    "                # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "                fused_scores[doc_str] += 1 / (rank + k)\n",
    "        # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "        reranked_results = [\n",
    "         (  doc,score )for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "        for doc , score in reranked_results:\n",
    "            print ( \"==================\")\n",
    "            print( f\" Score: {score}  === DOC: {doc}\")\n",
    "            print ( \"==================\")\n",
    "        # print(len(reranked_results))\n",
    "        return reranked_results\n",
    "\n",
    "   \n",
    "  \n",
    "    async def retrieve(self,query: str):\n",
    "\n",
    "    \n",
    "        docs = await asyncio.to_thread(\n",
    "                    self.vectorstore.similarity_search, query, 2\n",
    "                )\n",
    "        serialized = \"\\n\".join(d.page_content for d in docs)\n",
    "        \n",
    "        return serialized\n",
    "\n",
    "    def _build_tool(self):\n",
    "        \n",
    "        @tool(description=\"Retrieve documents using multi-query expansion.\")\n",
    "        async def retrieve(query: str):\n",
    "\n",
    "             # 1️⃣ Generate multiple queries\n",
    "\n",
    "            expansion_response = await self.llm_provider.base().ainvoke(\n",
    "                    self.MULTI_QUERY_PROMPT.format(question=query)\n",
    "                )\n",
    "\n",
    "            print(\"expansion_response: \",expansion_response)\n",
    "            queries = [\n",
    "                q.strip() for q in expansion_response.content.split(\"\\n\")\n",
    "                if q.strip()\n",
    "            ]\n",
    "            queries.append(query)\n",
    "\n",
    "             # 2️⃣ Retrieve docs for each query\n",
    "            all_docs = []\n",
    "            for q in queries:\n",
    "                #similarity_search() is blocking CPU-bound code\n",
    "                #Runs the function in a thread pool\n",
    "                #Frees the event loop\n",
    "                \n",
    "                docs = await asyncio.to_thread(\n",
    "                    self.vectorstore.similarity_search, q, 2\n",
    "                )\n",
    "                print(f\"Retrieved {docs} documents for query: {q}\")\n",
    "                all_docs.append(docs)\n",
    " \n",
    "\n",
    "            fused_docs = self.reciprocal_rank_fusion(all_docs)\n",
    "            \n",
    "            serialized = \"\\n\".join([doc[0] for doc in fused_docs[:3]])\n",
    "            # union_docs = self.get_unique_union(all_docs)\n",
    "            # serialized = \"\\n\".join(union_docs[:3])\n",
    "            print(\"serialized: \",serialized)\n",
    "            return serialized\n",
    "\n",
    "        return retrieve\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73ea003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph.message import  add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage, AIMessage, ToolMessage, HumanMessage\n",
    ")\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    # Conversation\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "    # Decomposition\n",
    "    sub_questions: List[str]\n",
    "    current_subq_index: int\n",
    "\n",
    "    # Memory across sub-questions\n",
    "    qa_pairs: List[str]\n",
    "\n",
    "    retrieved_context:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c993f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "ROUTER_PROMPT = \"\"\"\n",
    "You are a router for a question-answering system.\n",
    "\n",
    "Decide whether the user's question requires looking up\n",
    "information from documents or can be answered directly.\n",
    "\n",
    "Answer with ONLY one word:\n",
    "RETRIEVE or DIRECT\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "class RAGNodes:\n",
    "    def __init__(self, llm_provider : LLMProvider, retrieve_tool:RetrievalTool):\n",
    "        self.llm_provider = llm_provider\n",
    "        self.retrieve_tool = retrieve_tool\n",
    "\n",
    "    async def query_or_respond(self, state: GraphState):\n",
    "        \n",
    "        llm = self.llm_provider.with_tools(\n",
    "            [self.retrieve_tool.tool]\n",
    "          \n",
    "        )\n",
    "\n",
    "        messages = list(state[\"messages\"])\n",
    "        if not any(isinstance(m, SystemMessage) for m in messages):\n",
    "            messages.insert(0, SystemMessage(\n",
    "                \"Use the retrieve tool if external information is required.\"\n",
    "            ))\n",
    "        print_state(state, \"query_or_respond\")\n",
    "        print( \"state in query_or_respond: \",state , messages)\n",
    "        response =  await llm.ainvoke(messages)\n",
    "        print(\"Response llm: \", response)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    async def route(self, state: GraphState):\n",
    "        question = state[\"messages\"][0].content\n",
    "        print(state['messages'], question)\n",
    "        response = await self.llm_provider.base().ainvoke(\n",
    "        ROUTER_PROMPT.format(question=question)\n",
    "         )\n",
    "        \n",
    "        print(\"route decision: \",response.content)\n",
    "        decision = response.content.strip().upper()\n",
    "        if decision not in {\"RETRIEVE\", \"DIRECT\"}:\n",
    "            decision = \"RETRIEVE\"  # safe fallback\n",
    "        if decision == \"RETRIEVE\":\n",
    "            return \"decompose\"\n",
    "        return\"generate\"\n",
    "    \n",
    "    async def decompose(self, state: GraphState):\n",
    "            question = state[\"messages\"][0].content\n",
    "            # Decomposition\n",
    "            template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "            The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "            Generate multiple search queries related to: {question} \\n\n",
    "            Output (3 queries):\"\"\"\n",
    "\n",
    "            response =  await self.llm_provider.base().ainvoke(\n",
    "                template.format(question= question)\n",
    "            )\n",
    "            subqs = [q.strip() for q in response.content.split(\"\\n\") if q.strip()]\n",
    "\n",
    "\n",
    "            return  {\n",
    "            \"sub_questions\": subqs,\n",
    "            \"current_subq_index\": 0,\n",
    "            \"qa_pairs\": []\n",
    "            }\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    async def generate(self, state: GraphState):\n",
    "        tool_messages = []\n",
    "        print(\"Generating with retrieved context...\")\n",
    "        # print_state(state, \"generate\")\n",
    "        print( \"state in generate: \",state )\n",
    "        for msg in reversed(state[\"messages\"]):\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                tool_messages.append(msg)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        docs_context = \"\\n\".join(m.content for m in reversed(tool_messages))\n",
    "\n",
    "        system = SystemMessage(f\"\"\"\n",
    "            You are a helpful assistant.\n",
    "            Use the retrieved context if relevant.\n",
    "\n",
    "            Context:\n",
    "            ---------\n",
    "            {docs_context}\n",
    "            ---------\n",
    "            \"\"\")\n",
    "\n",
    "        convo = [\n",
    "            m for m in state[\"messages\"]\n",
    "            if isinstance(m, (HumanMessage, AIMessage))\n",
    "            and not getattr(m, \"tool_calls\", None)\n",
    "        ]\n",
    "\n",
    "        response =  await self.llm_provider.base().ainvoke(\n",
    "            [system, *convo]\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    async def retrieve(self, state: GraphState):\n",
    "        idx = state[\"current_subq_index\"]\n",
    "        subq = state[\"sub_questions\"][idx]\n",
    "        \n",
    "        print(\n",
    "          f\"idx: {idx} => subq: {subq} ==> qa_pairs: {state['qa_pairs'][:idx]}\"\n",
    "       )\n",
    "\n",
    "\n",
    "        serialized = await self.retrieve_tool.retrieve(query=subq)\n",
    "        \n",
    "        return {\n",
    "           \"retrieved_context\": serialized\n",
    "         }\n",
    "    \n",
    "    async def generate_decompose(self, state: GraphState):\n",
    "        idx = state[\"current_subq_index\"]\n",
    "        subq = state[\"sub_questions\"][idx]\n",
    "        q_a_pairs  = state.get(\"qa_pairs\", [])[:idx]\n",
    "\n",
    "\n",
    "        question = state[\"messages\"][0].content\n",
    "\n",
    "        # tool_msgs = [m for m in reversed(state[\"messages\"]) if isinstance(m, ToolMessage)][0]\n",
    "        # context = \"\\n\".join(m.content for m in tool_msgs)\n",
    "        context = state.get(\"retrieved_context\", \"\")\n",
    "        \n",
    "\n",
    "        template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "            \\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "            Here is any available background question + answer pairs:\n",
    "\n",
    "            \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "            Here is additional context relevant to the question: \n",
    "\n",
    "            \\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "            Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "            \"\"\"\n",
    "\n",
    "        answer = await self.llm_provider.base().ainvoke(template.format(question=subq,q_a_pairs=q_a_pairs, context= context ))\n",
    "        print(f\"AI Answer: {answer}\")\n",
    "        new_pair = f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "        \n",
    "        return {\n",
    "          \"qa_pairs\": state[\"qa_pairs\"] + [new_pair],\n",
    "          \"current_subq_index\": idx + 1,\n",
    "          \"messages\":[answer]\n",
    "         }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78bdaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "class RAGGraphBuilder:\n",
    "    def __init__(self, nodes: RAGNodes, retrieve_tool, checkpointer):\n",
    "        self.nodes = nodes\n",
    "        self.retrieve_tool = retrieve_tool\n",
    "        self.checkpointer = checkpointer\n",
    "    \n",
    "    @staticmethod\n",
    "    def tools_condition(state: GraphState):\n",
    "        last = state[\"messages\"][-1]\n",
    "        print_state(state, \"tools_condition\")\n",
    "        print( \"state in tools_condition: \",state )\n",
    "        if getattr(last, \"tool_calls\", None):\n",
    "            return \"tools\"\n",
    "        return END\n",
    "    @staticmethod\n",
    "    def should_continue(state: GraphState):\n",
    "        if state[\"current_subq_index\"] < len(state[\"sub_questions\"]) or state[\"current_subq_index\"] ==len(state[\"sub_questions\"])-1:\n",
    "            return \"retrieve\"\n",
    "        return END\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        graph = StateGraph(GraphState)\n",
    "\n",
    "        graph.add_node(\"queryOrResponse\", self.nodes.query_or_respond)\n",
    "        graph.add_node(\"decompose\", self.nodes.decompose)\n",
    "        # graph.add_node(\"tools\", ToolNode(tools=[self.retrieve_tool.tool]))\n",
    "        graph.add_node(\"generate\", self.nodes.generate)\n",
    "        graph.add_node(\"generateDecompose\",self.nodes.generate_decompose)\n",
    "        graph.add_node(\"retrieve\",self.nodes.retrieve)\n",
    "      \n",
    "        graph.add_edge(START, \"queryOrResponse\")\n",
    "        graph.add_conditional_edges(\"queryOrResponse\",  self.nodes.route, {\n",
    "            \"decompose\": \"decompose\",\n",
    "            \"generate\": \"generate\"\n",
    "        })\n",
    "        \n",
    "        # graph.add_edge(\"tools\", \"generate\")\n",
    "        graph.add_edge(\"decompose\", \"retrieve\")\n",
    "        graph.add_edge(\"retrieve\", \"generateDecompose\")\n",
    "        graph.add_conditional_edges(\n",
    "            \"generateDecompose\",\n",
    "            self.should_continue,\n",
    "            {\n",
    "                \"retrieve\": \"retrieve\",\n",
    "                END: END\n",
    "            }\n",
    "        )\n",
    "        graph.add_edge(\"generate\", END)\n",
    "\n",
    "        return graph.compile(checkpointer=self.checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578003d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "class RAGPipeline:\n",
    "    def __init__(self, api_key, checkpointer,vectorstore):\n",
    "        self.vectorstore =vectorstore\n",
    "        self.llm_provider = LLMProvider(api_key)\n",
    "        self.checkpointer = checkpointer\n",
    "\n",
    "    async def run(self, query, thread_id, user_id, pdf_id):\n",
    "\n",
    "        retrieve_tool = RetrievalTool(self.vectorstore, self.llm_provider)\n",
    "        nodes = RAGNodes(self.llm_provider, retrieve_tool)\n",
    "\n",
    "        graph = RAGGraphBuilder(\n",
    "            nodes, retrieve_tool, self.checkpointer\n",
    "        ).build()\n",
    "       \n",
    "\n",
    "        print(\"Running RAG graph...\", graph)\n",
    "        config: RunnableConfig = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "        result =  await graph.ainvoke(\n",
    "            {\"messages\": [HumanMessage(query)]},\n",
    "            config\n",
    "        )\n",
    "            \n",
    "        print(result)\n",
    "\n",
    "        return next(\n",
    "            (m.content for m in reversed(result[\"messages\"])\n",
    "             if isinstance(m, AIMessage)),\n",
    "            \"\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e2d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG graph... <langgraph.graph.state.CompiledStateGraph object at 0x7ca6dc8ca570>\n",
      "\n",
      "=== query_or_respond ===\n",
      "[0] HumanMessage: describe  table 2?\n",
      "==============\n",
      "\n",
      "state in query_or_respond:  {'messages': [HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='88ba23e4-2528-47a6-9784-51bc1cae9482')]} [SystemMessage(content='Use the retrieve tool if external information is required.', additional_kwargs={}, response_metadata={}), HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='88ba23e4-2528-47a6-9784-51bc1cae9482')]\n",
      "Response llm:  content='' additional_kwargs={'tool_calls': [{'id': 'v9cdtxxw9', 'function': {'arguments': '{\"query\":\"table 2\"}', 'name': 'retrieve'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 335, 'prompt_tokens': 161, 'total_tokens': 496, 'completion_time': 0.710114478, 'completion_tokens_details': {'reasoning_tokens': 311}, 'prompt_time': 0.006438659, 'prompt_tokens_details': None, 'queue_time': 0.051221861, 'total_time': 0.716553137}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b932f-c269-7822-b410-f4f9eeffe31d-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'table 2'}, 'id': 'v9cdtxxw9', 'type': 'tool_call'}] usage_metadata={'input_tokens': 161, 'output_tokens': 335, 'total_tokens': 496, 'output_token_details': {'reasoning': 311}}\n",
      "[HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='88ba23e4-2528-47a6-9784-51bc1cae9482'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'v9cdtxxw9', 'function': {'arguments': '{\"query\":\"table 2\"}', 'name': 'retrieve'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 335, 'prompt_tokens': 161, 'total_tokens': 496, 'completion_time': 0.710114478, 'completion_tokens_details': {'reasoning_tokens': 311}, 'prompt_time': 0.006438659, 'prompt_tokens_details': None, 'queue_time': 0.051221861, 'total_time': 0.716553137}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b932f-c269-7822-b410-f4f9eeffe31d-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'table 2'}, 'id': 'v9cdtxxw9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 335, 'total_tokens': 496, 'output_token_details': {'reasoning': 311}})] describe  table 2?\n",
      "route decision:  RETRIEVE\n",
      "idx: 0 => subq: 1. What are the key columns and their data types in Table 2? ==> qa_pairs: []\n",
      "AI Answer: content='The provided context does not explicitly describe **Table 2** or its structure. The only table mentioned in the context is **Table 1**, which outlines the multi-resolution support of the DeepEncoder, including columns like:\\n\\n- **Mode** (e.g., Tiny, Small, Base, Large, Gundam, Gundam-M)  \\n- **Native Resolution** (e.g., 512, 640, 1024, 1280)  \\n- **Dynamic Resolution** (e.g., 640+1024, 1024+1280)  \\n- **Tokens** (e.g., 64, 100, 256, 400, combinations like `n×100+256` or `n×256+400`)  \\n\\nSince **Table 2** is not referenced or detailed in the provided context, its key columns and data types **cannot be determined** from the given information.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1074, 'prompt_tokens': 1212, 'total_tokens': 2286, 'completion_time': 2.081498774, 'completion_tokens_details': {'reasoning_tokens': 856}, 'prompt_time': 0.057428845, 'prompt_tokens_details': None, 'queue_time': 0.050978595, 'total_time': 2.138927619}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b932f-ce2e-7d63-8d03-8c0342840fbe-0' usage_metadata={'input_tokens': 1212, 'output_tokens': 1074, 'total_tokens': 2286, 'output_token_details': {'reasoning': 856}}\n",
      "idx: 1 => subq: 2. What is the main purpose or focus of the data presented in Table 2? ==> qa_pairs: [\"Question: describe  table 2?\\nAnswer: content='The provided context does not explicitly describe **Table 2** or its structure. The only table mentioned in the context is **Table 1**, which outlines the multi-resolution support of the DeepEncoder, including columns like:\\\\n\\\\n- **Mode** (e.g., Tiny, Small, Base, Large, Gundam, Gundam-M)  \\\\n- **Native Resolution** (e.g., 512, 640, 1024, 1280)  \\\\n- **Dynamic Resolution** (e.g., 640+1024, 1024+1280)  \\\\n- **Tokens** (e.g., 64, 100, 256, 400, combinations like `n×100+256` or `n×256+400`)  \\\\n\\\\nSince **Table 2** is not referenced or detailed in the provided context, its key columns and data types **cannot be determined** from the given information.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1074, 'prompt_tokens': 1212, 'total_tokens': 2286, 'completion_time': 2.081498774, 'completion_tokens_details': {'reasoning_tokens': 856}, 'prompt_time': 0.057428845, 'prompt_tokens_details': None, 'queue_time': 0.050978595, 'total_time': 2.138927619}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b932f-ce2e-7d63-8d03-8c0342840fbe-0' usage_metadata={'input_tokens': 1212, 'output_tokens': 1074, 'total_tokens': 2286, 'output_token_details': {'reasoning': 856}}\\n\\n\"]\n",
      "AI Answer: content=\"The main purpose of the data presented in Table 2 is to evaluate and demonstrate the performance of the model's **optical context compression capabilities**, specifically focusing on how different compression ratios impact decoding precision. The table highlights that:  \\n- At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance.  \\n- Beyond 10×, precision declines due to factors like increased layout complexity in long documents and text blurring at lower resolutions.  \\n- Even at **20× compression**, precision remains around **60%**, suggesting optical context compression is a viable and efficient method for reducing token usage without significant overhead.  \\n\\nThis data emphasizes the potential of optical context compression as a promising research direction, leveraging vision-language models (VLMs) to achieve efficient token compression while maintaining acceptable accuracy.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 498, 'prompt_tokens': 1822, 'total_tokens': 2320, 'completion_time': 1.070357011, 'completion_tokens_details': {'reasoning_tokens': 324}, 'prompt_time': 0.106411277, 'prompt_tokens_details': None, 'queue_time': 0.161091392, 'total_time': 1.1767682879999999}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b932f-d99c-7ff2-8ada-ab74c2e82615-0' usage_metadata={'input_tokens': 1822, 'output_tokens': 498, 'total_tokens': 2320, 'output_token_details': {'reasoning': 324}}\n",
      "idx: 2 => subq: 3. Are there any notable trends, patterns, or summary statistics highlighted in Table 2? ==> qa_pairs: [\"Question: describe  table 2?\\nAnswer: content='The provided context does not explicitly describe **Table 2** or its structure. The only table mentioned in the context is **Table 1**, which outlines the multi-resolution support of the DeepEncoder, including columns like:\\\\n\\\\n- **Mode** (e.g., Tiny, Small, Base, Large, Gundam, Gundam-M)  \\\\n- **Native Resolution** (e.g., 512, 640, 1024, 1280)  \\\\n- **Dynamic Resolution** (e.g., 640+1024, 1024+1280)  \\\\n- **Tokens** (e.g., 64, 100, 256, 400, combinations like `n×100+256` or `n×256+400`)  \\\\n\\\\nSince **Table 2** is not referenced or detailed in the provided context, its key columns and data types **cannot be determined** from the given information.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1074, 'prompt_tokens': 1212, 'total_tokens': 2286, 'completion_time': 2.081498774, 'completion_tokens_details': {'reasoning_tokens': 856}, 'prompt_time': 0.057428845, 'prompt_tokens_details': None, 'queue_time': 0.050978595, 'total_time': 2.138927619}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b932f-ce2e-7d63-8d03-8c0342840fbe-0' usage_metadata={'input_tokens': 1212, 'output_tokens': 1074, 'total_tokens': 2286, 'output_token_details': {'reasoning': 856}}\\n\\n\", 'Question: describe  table 2?\\nAnswer: content=\"The main purpose of the data presented in Table 2 is to evaluate and demonstrate the performance of the model\\'s **optical context compression capabilities**, specifically focusing on how different compression ratios impact decoding precision. The table highlights that:  \\\\n- At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance.  \\\\n- Beyond 10×, precision declines due to factors like increased layout complexity in long documents and text blurring at lower resolutions.  \\\\n- Even at **20× compression**, precision remains around **60%**, suggesting optical context compression is a viable and efficient method for reducing token usage without significant overhead.  \\\\n\\\\nThis data emphasizes the potential of optical context compression as a promising research direction, leveraging vision-language models (VLMs) to achieve efficient token compression while maintaining acceptable accuracy.\" additional_kwargs={} response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 498, \\'prompt_tokens\\': 1822, \\'total_tokens\\': 2320, \\'completion_time\\': 1.070357011, \\'completion_tokens_details\\': {\\'reasoning_tokens\\': 324}, \\'prompt_time\\': 0.106411277, \\'prompt_tokens_details\\': None, \\'queue_time\\': 0.161091392, \\'total_time\\': 1.1767682879999999}, \\'model_name\\': \\'qwen/qwen3-32b\\', \\'system_fingerprint\\': \\'fp_2bfcc54d36\\', \\'service_tier\\': \\'on_demand\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None, \\'model_provider\\': \\'groq\\'} id=\\'lc_run--019b932f-d99c-7ff2-8ada-ab74c2e82615-0\\' usage_metadata={\\'input_tokens\\': 1822, \\'output_tokens\\': 498, \\'total_tokens\\': 2320, \\'output_token_details\\': {\\'reasoning\\': 324}}\\n\\n']\n",
      "AI Answer: content=\"**Answer:**  \\nTable 2 highlights key trends and patterns related to the model's **optical context compression capabilities**, focusing on how compression ratios impact decoding precision. Notable observations include:  \\n\\n1. **High Precision at 10× Compression**:  \\n   - At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance for efficient token compression.  \\n\\n2. **Performance Decline Beyond 10×**:  \\n   - Precision decreases when compression ratios exceed 10×, attributed to:  \\n     - Increased layout complexity in long documents.  \\n     - Text blurring at lower resolutions (e.g., 512×512 or 640×640).  \\n\\n3. **Acceptable Performance at 20× Compression**:  \\n   - Even at **20× compression**, precision remains around **60%**, suggesting optical context compression remains viable for significant token reduction with minimal overhead.  \\n\\n4. **Efficiency and Viability**:  \\n   - The approach leverages existing vision-language model (VLM) infrastructure, introducing **no additional overhead**, making it a promising direction for research.  \\n\\nThese trends underscore the potential of optical context compression as an effective method for reducing token usage while maintaining reasonable accuracy, particularly for applications where moderate compression ratios are sufficient.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 779, 'prompt_tokens': 3658, 'total_tokens': 4437, 'completion_time': 1.7360365500000001, 'completion_tokens_details': {'reasoning_tokens': 501}, 'prompt_time': 0.188441263, 'prompt_tokens_details': None, 'queue_time': 0.162127486, 'total_time': 1.924477813}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b932f-e127-7c43-892e-62bef701a752-0' usage_metadata={'input_tokens': 3658, 'output_tokens': 779, 'total_tokens': 4437, 'output_token_details': {'reasoning': 501}}\n",
      "{'messages': [HumanMessage(content='describe  table 2?', additional_kwargs={}, response_metadata={}, id='88ba23e4-2528-47a6-9784-51bc1cae9482'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'v9cdtxxw9', 'function': {'arguments': '{\"query\":\"table 2\"}', 'name': 'retrieve'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 335, 'prompt_tokens': 161, 'total_tokens': 496, 'completion_time': 0.710114478, 'completion_tokens_details': {'reasoning_tokens': 311}, 'prompt_time': 0.006438659, 'prompt_tokens_details': None, 'queue_time': 0.051221861, 'total_time': 0.716553137}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b932f-c269-7822-b410-f4f9eeffe31d-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'table 2'}, 'id': 'v9cdtxxw9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 335, 'total_tokens': 496, 'output_token_details': {'reasoning': 311}}), AIMessage(content='The provided context does not explicitly describe **Table 2** or its structure. The only table mentioned in the context is **Table 1**, which outlines the multi-resolution support of the DeepEncoder, including columns like:\\n\\n- **Mode** (e.g., Tiny, Small, Base, Large, Gundam, Gundam-M)  \\n- **Native Resolution** (e.g., 512, 640, 1024, 1280)  \\n- **Dynamic Resolution** (e.g., 640+1024, 1024+1280)  \\n- **Tokens** (e.g., 64, 100, 256, 400, combinations like `n×100+256` or `n×256+400`)  \\n\\nSince **Table 2** is not referenced or detailed in the provided context, its key columns and data types **cannot be determined** from the given information.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1074, 'prompt_tokens': 1212, 'total_tokens': 2286, 'completion_time': 2.081498774, 'completion_tokens_details': {'reasoning_tokens': 856}, 'prompt_time': 0.057428845, 'prompt_tokens_details': None, 'queue_time': 0.050978595, 'total_time': 2.138927619}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b932f-ce2e-7d63-8d03-8c0342840fbe-0', usage_metadata={'input_tokens': 1212, 'output_tokens': 1074, 'total_tokens': 2286, 'output_token_details': {'reasoning': 856}}), AIMessage(content=\"The main purpose of the data presented in Table 2 is to evaluate and demonstrate the performance of the model's **optical context compression capabilities**, specifically focusing on how different compression ratios impact decoding precision. The table highlights that:  \\n- At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance.  \\n- Beyond 10×, precision declines due to factors like increased layout complexity in long documents and text blurring at lower resolutions.  \\n- Even at **20× compression**, precision remains around **60%**, suggesting optical context compression is a viable and efficient method for reducing token usage without significant overhead.  \\n\\nThis data emphasizes the potential of optical context compression as a promising research direction, leveraging vision-language models (VLMs) to achieve efficient token compression while maintaining acceptable accuracy.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 498, 'prompt_tokens': 1822, 'total_tokens': 2320, 'completion_time': 1.070357011, 'completion_tokens_details': {'reasoning_tokens': 324}, 'prompt_time': 0.106411277, 'prompt_tokens_details': None, 'queue_time': 0.161091392, 'total_time': 1.1767682879999999}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b932f-d99c-7ff2-8ada-ab74c2e82615-0', usage_metadata={'input_tokens': 1822, 'output_tokens': 498, 'total_tokens': 2320, 'output_token_details': {'reasoning': 324}}), AIMessage(content=\"**Answer:**  \\nTable 2 highlights key trends and patterns related to the model's **optical context compression capabilities**, focusing on how compression ratios impact decoding precision. Notable observations include:  \\n\\n1. **High Precision at 10× Compression**:  \\n   - At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance for efficient token compression.  \\n\\n2. **Performance Decline Beyond 10×**:  \\n   - Precision decreases when compression ratios exceed 10×, attributed to:  \\n     - Increased layout complexity in long documents.  \\n     - Text blurring at lower resolutions (e.g., 512×512 or 640×640).  \\n\\n3. **Acceptable Performance at 20× Compression**:  \\n   - Even at **20× compression**, precision remains around **60%**, suggesting optical context compression remains viable for significant token reduction with minimal overhead.  \\n\\n4. **Efficiency and Viability**:  \\n   - The approach leverages existing vision-language model (VLM) infrastructure, introducing **no additional overhead**, making it a promising direction for research.  \\n\\nThese trends underscore the potential of optical context compression as an effective method for reducing token usage while maintaining reasonable accuracy, particularly for applications where moderate compression ratios are sufficient.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 779, 'prompt_tokens': 3658, 'total_tokens': 4437, 'completion_time': 1.7360365500000001, 'completion_tokens_details': {'reasoning_tokens': 501}, 'prompt_time': 0.188441263, 'prompt_tokens_details': None, 'queue_time': 0.162127486, 'total_time': 1.924477813}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b932f-e127-7c43-892e-62bef701a752-0', usage_metadata={'input_tokens': 3658, 'output_tokens': 779, 'total_tokens': 4437, 'output_token_details': {'reasoning': 501}})], 'sub_questions': ['1. What are the key columns and their data types in Table 2?', '2. What is the main purpose or focus of the data presented in Table 2?', '3. Are there any notable trends, patterns, or summary statistics highlighted in Table 2?'], 'current_subq_index': 3, 'qa_pairs': [\"Question: describe  table 2?\\nAnswer: content='The provided context does not explicitly describe **Table 2** or its structure. The only table mentioned in the context is **Table 1**, which outlines the multi-resolution support of the DeepEncoder, including columns like:\\\\n\\\\n- **Mode** (e.g., Tiny, Small, Base, Large, Gundam, Gundam-M)  \\\\n- **Native Resolution** (e.g., 512, 640, 1024, 1280)  \\\\n- **Dynamic Resolution** (e.g., 640+1024, 1024+1280)  \\\\n- **Tokens** (e.g., 64, 100, 256, 400, combinations like `n×100+256` or `n×256+400`)  \\\\n\\\\nSince **Table 2** is not referenced or detailed in the provided context, its key columns and data types **cannot be determined** from the given information.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1074, 'prompt_tokens': 1212, 'total_tokens': 2286, 'completion_time': 2.081498774, 'completion_tokens_details': {'reasoning_tokens': 856}, 'prompt_time': 0.057428845, 'prompt_tokens_details': None, 'queue_time': 0.050978595, 'total_time': 2.138927619}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b932f-ce2e-7d63-8d03-8c0342840fbe-0' usage_metadata={'input_tokens': 1212, 'output_tokens': 1074, 'total_tokens': 2286, 'output_token_details': {'reasoning': 856}}\\n\\n\", 'Question: describe  table 2?\\nAnswer: content=\"The main purpose of the data presented in Table 2 is to evaluate and demonstrate the performance of the model\\'s **optical context compression capabilities**, specifically focusing on how different compression ratios impact decoding precision. The table highlights that:  \\\\n- At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance.  \\\\n- Beyond 10×, precision declines due to factors like increased layout complexity in long documents and text blurring at lower resolutions.  \\\\n- Even at **20× compression**, precision remains around **60%**, suggesting optical context compression is a viable and efficient method for reducing token usage without significant overhead.  \\\\n\\\\nThis data emphasizes the potential of optical context compression as a promising research direction, leveraging vision-language models (VLMs) to achieve efficient token compression while maintaining acceptable accuracy.\" additional_kwargs={} response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 498, \\'prompt_tokens\\': 1822, \\'total_tokens\\': 2320, \\'completion_time\\': 1.070357011, \\'completion_tokens_details\\': {\\'reasoning_tokens\\': 324}, \\'prompt_time\\': 0.106411277, \\'prompt_tokens_details\\': None, \\'queue_time\\': 0.161091392, \\'total_time\\': 1.1767682879999999}, \\'model_name\\': \\'qwen/qwen3-32b\\', \\'system_fingerprint\\': \\'fp_2bfcc54d36\\', \\'service_tier\\': \\'on_demand\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None, \\'model_provider\\': \\'groq\\'} id=\\'lc_run--019b932f-d99c-7ff2-8ada-ab74c2e82615-0\\' usage_metadata={\\'input_tokens\\': 1822, \\'output_tokens\\': 498, \\'total_tokens\\': 2320, \\'output_token_details\\': {\\'reasoning\\': 324}}\\n\\n', 'Question: describe  table 2?\\nAnswer: content=\"**Answer:**  \\\\nTable 2 highlights key trends and patterns related to the model\\'s **optical context compression capabilities**, focusing on how compression ratios impact decoding precision. Notable observations include:  \\\\n\\\\n1. **High Precision at 10× Compression**:  \\\\n   - At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance for efficient token compression.  \\\\n\\\\n2. **Performance Decline Beyond 10×**:  \\\\n   - Precision decreases when compression ratios exceed 10×, attributed to:  \\\\n     - Increased layout complexity in long documents.  \\\\n     - Text blurring at lower resolutions (e.g., 512×512 or 640×640).  \\\\n\\\\n3. **Acceptable Performance at 20× Compression**:  \\\\n   - Even at **20× compression**, precision remains around **60%**, suggesting optical context compression remains viable for significant token reduction with minimal overhead.  \\\\n\\\\n4. **Efficiency and Viability**:  \\\\n   - The approach leverages existing vision-language model (VLM) infrastructure, introducing **no additional overhead**, making it a promising direction for research.  \\\\n\\\\nThese trends underscore the potential of optical context compression as an effective method for reducing token usage while maintaining reasonable accuracy, particularly for applications where moderate compression ratios are sufficient.\" additional_kwargs={} response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 779, \\'prompt_tokens\\': 3658, \\'total_tokens\\': 4437, \\'completion_time\\': 1.7360365500000001, \\'completion_tokens_details\\': {\\'reasoning_tokens\\': 501}, \\'prompt_time\\': 0.188441263, \\'prompt_tokens_details\\': None, \\'queue_time\\': 0.162127486, \\'total_time\\': 1.924477813}, \\'model_name\\': \\'qwen/qwen3-32b\\', \\'system_fingerprint\\': \\'fp_2bfcc54d36\\', \\'service_tier\\': \\'on_demand\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None, \\'model_provider\\': \\'groq\\'} id=\\'lc_run--019b932f-e127-7c43-892e-62bef701a752-0\\' usage_metadata={\\'input_tokens\\': 3658, \\'output_tokens\\': 779, \\'total_tokens\\': 4437, \\'output_token_details\\': {\\'reasoning\\': 501}}\\n\\n'], 'retrieved_context': 'Dolphin[11] - 0.356 0.352 0.465 0.258 0.35 0.44 0.44 0.604 0.367 0.351\\nMarker[1] - 0.296 0.085 0.374 0.609 0.116 0.497 0.293 0.688 0.678 0.329\\nMathpix[2] - 0.191 0.105 0.306 0.243 0.108 0.364 0.381 0.454 0.32 0.30\\nMinerU-2.1.1[34] - 0.162 0.072 0.313 0.166 0.097 0.244 0.111 0.581 0.15 0.136\\nMonkeyOCR-1.2B[18] - 0.154 0.062 0.295 0.164 0.094 0.263 0.179 0.464 0.168 0.243\\nPPstructure-v3 [9] - 0.152 0.073 0.295 0.162 0.077 0.223 0.136 0.535 0.111 0.11\\nEnd-to-end Models\\nNougat[6] 2352 0.452 0.365 0.488 0.572 0.382 0.973 0.998 0.941 1.00 0.954\\nSmolDocling [25] 392 0.493 0.262 0.753 0.729 0.227 0.816 0.838 0.997 0.907 0.522\\nInternVL2-76B[8] 6790 0.44 0.353 0.543 0.547 0.317 0.443 0.29 0.701 0.555 0.228\\nQwen2.5-VL-7B[5] 3949 0.316 0.151 0.376 0.598 0.138 0.399 0.243 0.5 0.627 0.226\\nOLMOCR[28] 3949 0.326 0.097 0.455 0.608 0.145 0.469 0.293 0.655 0.652 0.277\\nGOT-OCR2.0 [38] 256 0.287 0.189 0.360 0.459 0.141 0.411 0.315 0.528 0.52 0.28\\nOCRFlux-3B[3] 3949 0.238 0.112 0.447 0.269 0.126 0.349 0.256 0.716 0.162 0.263\\nGPT4o[26] - 0.233 0.144 0.425 0.234 0.128 0.399 0.409 0.606 0.329 0.251\\nInternVL3-78B [42] 6790 0.218 0.117 0.38 0.279 0.095 0.296 0.21 0.533 0.282 0.161\\nQwen2.5-VL-72B[5] 3949 0.214 0.092 0.315 0.341 0.106 0.261 0.18 0.434 0.262 0.168\\ndots.ocr[30] 3949 0.182 0.137 0.320 0.166 0.182 0.261 0.229 0.468 0.160 0.261\\nGemini2.5-Pro [4] - 0.148 0.055 0.356 0.13 0.049 0.212 0.168 0.439 0.119 0.121\\nMinerU2.0[34] 6790 0.133 0.045 0.273 0.15 0.066 0.238 0.115 0.506 0.209 0.122\\ndots.ocr†200dpi[30] 5545 0.1250.0320.3290.099 0.040.160.0660.416 0.0920.067\\nDeepSeek-OCR (end2end)\\nTiny640.386 0.373 0.469 0.422 0.283 0.361 0.307 0.635 0.266 0.236\\nSmall 100 0.221 0.142 0.373 0.242 0.125 0.284 0.24 0.53 0.159 0.205\\nBase 256(182) 0.137 0.054 0.267 0.163 0.064 0.24 0.205 0.474 0.1 0.181\\nLarge 400(285) 0.138 0.054 0.277 0.152 0.067 0.208 0.143 0.461 0.104 0.123\\nGundam 795 0.127 0.043 0.269 0.134 0.062 0.181 0.097 0.432 0.089 0.103\\nLarge 400(285) 0.138 0.054 0.277 0.152 0.067 0.208 0.143 0.461 0.104 0.123\\nGundam 795 0.127 0.043 0.269 0.134 0.062 0.181 0.097 0.432 0.089 0.103\\nGundam-M†200dpi 18530.1230.0490.2420.147 0.0560.1570.0870.377 0.080.085\\nwithout layout: \"<image>\\\\nFree OCR.\"to control the model’s output format.Nevertheless, the\\noutput format still cannot completely match Fox benchmarks, so the actual performance would\\nbe somewhat higher than the test results.As shown in Table 2, within a 10×compression ratio, the model’s decoding precision can\\nreach approximately 97%, which is a very promising result.In the future, it may be possible to\\nachieve nearly 10×lossless contexts compression through text-to-image approaches.When the\\ncompression ratio exceeds 10×, performance begins to decline, which may have two reasons:\\none is that the layout of long documents becomes more complex, and another reason may be\\nthat long texts become blurred at 512×512 or 640×640 resolution.The first issue can be solved\\nby rendering texts onto a single layout page, while we believe the second issue will become\\n11 a feature of the forgetting mechanism.When compressing tokens by nearly 20×, we find that\\nprecision can still approach 60%.These results indicate that optical contexts compression is\\na very promising and worthwhile research direction, and this approach does not bring any\\noverhead because it can leverage VLM infrastructure, as multimodal systems inherently require\\nan additional vision encoder.Table 4 |Edit distances for different categories of documents in OmniDocBench.The results\\nshow that some types of documents can achieve good performance with just 64 or 100 vision\\ntokens, while others require Gundam mode.Mode\\nType Book Slides Financial\\nReport Textbook Exam\\nPaper Magazine Academic\\nPapers Notes Newspaper Overall\\nTiny 0.147 0.116 0.207 0.173 0.294 0.201 0.395 0.297 0.94 0.32\\nSmall 0.085 0.111 0.079 0.147 0.171 0.107 0.131 0.187 0.744 0.205\\nBase 0.037 0.08 0.027 0.1 0.13 0.073 0.052 0.176 0.645 0.156'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"**Answer:**  \\nTable 2 highlights key trends and patterns related to the model's **optical context compression capabilities**, focusing on how compression ratios impact decoding precision. Notable observations include:  \\n\\n1. **High Precision at 10× Compression**:  \\n   - At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance for efficient token compression.  \\n\\n2. **Performance Decline Beyond 10×**:  \\n   - Precision decreases when compression ratios exceed 10×, attributed to:  \\n     - Increased layout complexity in long documents.  \\n     - Text blurring at lower resolutions (e.g., 512×512 or 640×640).  \\n\\n3. **Acceptable Performance at 20× Compression**:  \\n   - Even at **20× compression**, precision remains around **60%**, suggesting optical context compression remains viable for significant token reduction with minimal overhead.  \\n\\n4. **Efficiency and Viability**:  \\n   - The approach leverages existing vision-language model (VLM) infrastructure, introducing **no additional overhead**, making it a promising direction for research.  \\n\\nThese trends underscore the potential of optical context compression as an effective method for reducing token usage while maintaining reasonable accuracy, particularly for applications where moderate compression ratios are sufficient.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = RAGPipeline(\n",
    "    api_key=API_KEY_GROQ,\n",
    "    checkpointer=checkpointer,\n",
    "    vectorstore=memory_store\n",
    ")\n",
    "result =  await pipeline.run(\"describe  table 2?\", thread_id=\"thread1\", user_id=\"user1\", pdf_id=\"pdf1\" )\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d653bb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answer:** **Answer:**  \n",
       "Table 2 highlights key trends and patterns related to the model's **optical context compression capabilities**, focusing on how compression ratios impact decoding precision. Notable observations include:  \n",
       "\n",
       "1. **High Precision at 10× Compression**:  \n",
       "   - At a **10× compression ratio**, the model achieves approximately **97% decoding precision**, indicating strong performance for efficient token compression.  \n",
       "\n",
       "2. **Performance Decline Beyond 10×**:  \n",
       "   - Precision decreases when compression ratios exceed 10×, attributed to:  \n",
       "     - Increased layout complexity in long documents.  \n",
       "     - Text blurring at lower resolutions (e.g., 512×512 or 640×640).  \n",
       "\n",
       "3. **Acceptable Performance at 20× Compression**:  \n",
       "   - Even at **20× compression**, precision remains around **60%**, suggesting optical context compression remains viable for significant token reduction with minimal overhead.  \n",
       "\n",
       "4. **Efficiency and Viability**:  \n",
       "   - The approach leverages existing vision-language model (VLM) infrastructure, introducing **no additional overhead**, making it a promising direction for research.  \n",
       "\n",
       "These trends underscore the potential of optical context compression as an effective method for reducing token usage while maintaining reasonable accuracy, particularly for applications where moderate compression ratios are sufficient."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"**Answer:** {result}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9acb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGoogleGenerativeAI(\n",
    "#             model=\"gemini-2.5-flash\",\n",
    "#             temperature=0,\n",
    "#             api_key=api_key1,\n",
    "#         )\n",
    "\n",
    "# llm.invoke(ROUTER_PROMPT.format(question=\"What is the contribution of the paper ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14957d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea120b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = memory_store.similarity_search(\"ocr\", k=3)\n",
    "\n",
    "# print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dadc2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EuroEval/gemma-3-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b8949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def get_chunks_with_ids(text):\n",
    "#     cleaned_text = re.sub(r\"(Contents).*?1\\. Introduction\", r\"\\1\\n\", text, flags=re.DOTALL)\n",
    "#     cleaned_text = re.sub(r\"(References|REFERENCES).*?$\", \"\", cleaned_text, flags=re.DOTALL)\n",
    "#     sentences = split_sentences(cleaned_text)\n",
    "#     text_sentences = [s for s in sentences if maybe_is_text(s)]\n",
    "\n",
    "#     currentIndex = 0\n",
    "#     chunks_with_ids = []\n",
    "#     for i, s in enumerate(text_sentences):\n",
    "#         start = currentIndex\n",
    "#         end = start + len(s)\n",
    "\n",
    "#         chunks_with_ids.append((i, start, end, s))\n",
    "#         currentIndex = end+1  # +1 for the space/newline between sentences\n",
    "    \n",
    "\n",
    "#     return chunks_with_ids\n",
    "        \n",
    "\n",
    "\n",
    "# def createChunksPreservingId (chunks_with_ids, tokenizer, maxTokens, buffer):\n",
    "#     chunks = []\n",
    "#     currentTokens =0\n",
    "#     currentChunk = []\n",
    "\n",
    "#     for chunk in chunks_with_ids:\n",
    "#         tokenCount = len(tokenizer.encode(chunk[3], add_special_tokens=False))\n",
    "#         if currentTokens + tokenCount +buffer > maxTokens:\n",
    "#             if  len(currentChunk) > 0:\n",
    "#                chunks.append([currentChunk])\n",
    "#             currentChunk = [chunk]\n",
    "#             currentTokens = tokenCount\n",
    "\n",
    "#         else:\n",
    "#             currentChunk.append(chunk)\n",
    "#             currentTokens += tokenCount\n",
    "#     if  len(currentChunk) > 0:\n",
    "#                chunks.append([currentChunk])\n",
    "#     return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b5b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_ids = get_chunks_with_ids(cleaned_text)\n",
    "# chunks = createChunksPreservingId(sentences_ids, tokenizer, maxTokens=500, buffer=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
